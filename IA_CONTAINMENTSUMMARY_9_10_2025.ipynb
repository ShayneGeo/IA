{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5d4e2-0324-466d-bb38-df7d7ebd9e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba67ad-058d-4799-920b-d59aeec89435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "IA analysis \n",
    "100 in timber and 300 acre in grasslands \n",
    "\n",
    "CITATION FOR THIS LOGIC:\n",
    "*!*!\n",
    "https://www.nifc.gov/sites/default/files/redbook-files/Chapter11.pdf\n",
    "*!*!\n",
    "\n",
    "Fire-size class definitions (by final fire perimeter acres)\n",
    "-----------------------------------------------------------\n",
    "A: > 0 and <= 0.25 acres\n",
    "B: >= 0.26 and <= 9.9 acres\n",
    "C: >= 10.0 and <= 99.9 acres\n",
    "D: >= 100 and <= 299 acres\n",
    "E: >= 300 and <= 999 acres\n",
    "F: >= 1000 and <= 4999 acres\n",
    "G: >= 5000 acres\n",
    "\n",
    "Rules for `event_type`:\n",
    "- If fire occurs in land cover types [52 (Shrubland), 71 (Grassland), 81 (Pasture/Hay)] (300 acres in grasslands ):\n",
    "    - Assign event_type = 1 only if FIRE_SIZE_CLASS is in ['E', 'F', 'G'] (larger fires).\n",
    "\n",
    "- If fire occurs in any other land cover (100 acres in forests):\n",
    "    - Assign event_type = 1 if FIRE_SIZE_CLASS is in ['D', 'E', 'F', 'G'].\n",
    "\n",
    "- Otherwise assign event_type = 0.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Read CSV\n",
    "# df = pd.read_csv(r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\2020_FPA_FOD_cons.csv\")\n",
    "\n",
    "# # Display head\n",
    "# # print(df.head())\n",
    "\n",
    "# # Create binary event_type column based on FIRE_SIZE_CLASS\n",
    "# #df['event_type'] = df['FIRE_SIZE_CLASS'].apply(lambda x: 1 if x in ['D', 'E', 'F', 'G'] else 0)\n",
    "# df['event_type'] = df.apply(\n",
    "#     lambda row: 1 if (row['Land_Cover'] in [52, 71, 81] and row['FIRE_SIZE_CLASS'] in ['E', 'F', 'G']) \n",
    "#     or (row['Land_Cover'] not in [52, 71, 81] and row['FIRE_SIZE_CLASS'] in ['D', 'E', 'F', 'G']) \n",
    "#     else 0,\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# Display head\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_path = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\FPA_FOD_Plus.csv\"\n",
    "\n",
    "# How many rows to sample (e.g., 1% of the file)\n",
    "frac =  1#0.001  \n",
    "\n",
    "# Count lines quickly\n",
    "n_lines = sum(1 for _ in open(csv_path))\n",
    "\n",
    "# Build random skip mask (keep header row)\n",
    "skip = sorted(np.random.choice(np.arange(1, n_lines), size=int((n_lines-1)*(1-frac)), replace=False))\n",
    "\n",
    "df_FULL = pd.read_csv(csv_path, skiprows=skip)\n",
    "\n",
    "print(df_FULL.shape)\n",
    "\n",
    "# Create binary event_type column based on FIRE_SIZE_CLASS\n",
    "#df['event_type'] = df['FIRE_SIZE_CLASS'].apply(lambda x: 1 if x in ['D', 'E', 'F', 'G'] else 0)\n",
    "df_FULL['event_type'] = df_FULL.apply(\n",
    "    lambda row: 1 if (row['Land_Cover'] in [52, 71, 81] and row['FIRE_SIZE_CLASS'] in ['E', 'F', 'G']) \n",
    "    or (row['Land_Cover'] not in [52, 71, 81] and row['FIRE_SIZE_CLASS'] in ['D', 'E', 'F', 'G']) \n",
    "    else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(len(df_FULL))\n",
    "\n",
    "df_SUB = df_FULL[df_FULL[\"NWCG_REPORTING_AGENCY\"].isin([\"BLM\", \"NPS\", \"FS\"])]\n",
    "print(df_SUB[\"NWCG_REPORTING_AGENCY\"].value_counts())\n",
    "print(len(df_SUB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540cfb5-404a-4761-8e12-75c83176e03e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2255aef-be63-44cd-8728-6ff5edd34ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SUB = df_FULL[df_FULL[\"NWCG_REPORTING_AGENCY\"].isin([\"BLM\", \"NPS\", \"FS\"])]\n",
    "# print(df_SUB[\"NWCG_REPORTING_AGENCY\"].value_counts())\n",
    "# print(len(df_SUB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a88e6-dd5a-4586-a82a-74a2248604d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f8361-eead-4975-9459-fbdbcd046a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique counts for GACCAbbrev\n",
    "print(\"Value counts (including NaN):\")\n",
    "print(df_SUB[\"GACCAbbrev\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nUnique values:\")\n",
    "print(df_SUB[\"GACCAbbrev\"].unique())\n",
    "\n",
    "print(\"\\nNumber of unique values (including NaN):\")\n",
    "print(df_SUB[\"GACCAbbrev\"].nunique(dropna=False))\n",
    "\n",
    "# Check for \"wrong\"/unexpected values\n",
    "valid_set = {\"ONCC\",\"GBCC\",\"NWCC\",\"SWCC\",\"NRCC\",\"AICC\",\"SACC\",\"RMCC\",\"OSCC\",\"EACC\"}\n",
    "mask_invalid = ~df_SUB[\"GACCAbbrev\"].isin(valid_set) & df_SUB[\"GACCAbbrev\"].notna()\n",
    "\n",
    "if mask_invalid.any():\n",
    "    print(\"\\n⚠️ Unexpected GACCAbbrev values found:\")\n",
    "    print(df_SUB.loc[mask_invalid, \"GACCAbbrev\"].value_counts())\n",
    "else:\n",
    "    print(\"\\nAll non-null GACCAbbrev values are in the expected set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b572b4-358b-4e6a-8d75-41f8d572b8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729fcf1-7779-48e4-b102-90b3203369fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa60f74-745a-4fa9-9ccc-397262d0f1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73e41d-56c5-46dc-a1fb-b6f6b93313f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Start from merged_final and ensure no sjoin-reserved columns are present ---\n",
    "df = df_SUB.copy()\n",
    "for col in [\"index_right\", \"index_left\"]:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=col)\n",
    "\n",
    "# --- Keep only rows with valid coords ---\n",
    "mask_xy = df[\"LATITUDE\"].notna() & df[\"LONGITUDE\"].notna()\n",
    "df_xy = df.loc[mask_xy].copy()\n",
    "\n",
    "# --- Build point GeoDataFrame ---\n",
    "gdf_points = gpd.GeoDataFrame(\n",
    "    df_xy,\n",
    "    geometry=gpd.points_from_xy(df_xy[\"LONGITUDE\"], df_xy[\"LATITUDE\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# --- Load GACC polygons ---\n",
    "gacc_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\National_GACC_Final_20250113.shp\"\n",
    "gdf_gacc = gpd.read_file(gacc_fp)[[\"GACCAbbrev\", \"geometry\"]].reset_index(drop=True)\n",
    "\n",
    "# --- Reproject if needed ---\n",
    "if gdf_gacc.crs != gdf_points.crs:\n",
    "    gdf_gacc = gdf_gacc.to_crs(gdf_points.crs)\n",
    "\n",
    "# --- Spatial join (avoid 'index_right' collision via suffixes) ---\n",
    "gdf_joined = gpd.sjoin(\n",
    "    gdf_points.reset_index(),               # keep original index to map back\n",
    "    gdf_gacc,\n",
    "    how=\"left\",\n",
    "    predicate=\"within\",\n",
    "    lsuffix=\"PTS\",\n",
    "    rsuffix=\"GACC\"\n",
    ")\n",
    "\n",
    "# --- Extract GACCAbbrev as GACCTRUTH and map back to full dataframe ---\n",
    "gacc_map = gdf_joined.set_index(\"index\")[\"GACCAbbrev\"]\n",
    "df[\"GACCTRUTH\"] = np.nan\n",
    "df.loc[gacc_map.index, \"GACCTRUTH\"] = gacc_map.values\n",
    "\n",
    "# --- Put result back into merged_final ---\n",
    "df_SUB = df\n",
    "\n",
    "# Optional quick check:\n",
    "# print(merged_final[\"GACCTRUTH\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdf940-4fd7-4560-a7af-bd0b2c96aeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de85f7e-fa0a-4e16-b31b-aa14d459482b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5262e5-ae37-4a2d-b8e0-deb976d7dc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bba15b-db23-4084-8262-0d68c25d3a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3410d38-74f1-451d-8aae-7187cdb10801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0703a34-58b4-4a5a-86cc-672e7d0872e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script summarizes initial-attack (IA) outcomes from a DataFrame that already has event_type coded (0 = IA success, 1 = IA failure). \n",
    "It first prints overall counts and percentages of successes and failures, then builds a bar chart showing total counts with \n",
    "percentage labels for each outcome. If the GACCAbbrev column exists, it groups by GACC, computes each region’s success/failure counts and shares, \n",
    "sorts regions by success share, and plots a stacked bar chart (success on bottom, failure on top) with per-region success percentages labeled above the bars. \n",
    "If GACCAbbrev is missing, it simply skips the by-GACC plots.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumes df already exists with column 'event_type' (0=IA success, 1=IA failure)\n",
    "# If not, uncomment and set your CSV path:\n",
    "# df = pd.read_csv(r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\2020_FPA_FOD_cons.csv\")\n",
    "\n",
    "df= df_SUB\n",
    "\n",
    "# ---------- 1) Print success/failure ratio ----------\n",
    "total = len(df)\n",
    "n_success = int((df[\"event_type\"] == 0).sum())\n",
    "n_failure = int((df[\"event_type\"] == 1).sum())\n",
    "print(f\"IA Success (0) / Total: {n_success} / {total}  ({(n_success/total*100):.2f}%)\")\n",
    "print(f\"IA Failure (1) / Total: {n_failure} / {total}  ({(n_failure/total*100):.2f}%)\")\n",
    "\n",
    "# ---------- 2) Overall counts + percentages ----------\n",
    "counts = df[\"event_type\"].value_counts().reindex([0,1]).fillna(0).astype(int)\n",
    "props = counts / counts.sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4.5))\n",
    "bars = ax.bar([\"IA Success\", \"IA Failure\"], counts.values, color=[\"#2ca02c\", \"#d62728\"])\n",
    "for rect, c, p in zip(bars, counts.values, props.values):\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, rect.get_height() * 1.01,\n",
    "            f\"{c:,}\\n({p*100:.1f}%)\", ha=\"center\", va=\"bottom\", fontsize=12)\n",
    "#ax.set_title(\"Initial Attack Outcomes (Counts with %)\")\n",
    "#ax.set_ylabel(\"Number of Fires\")\n",
    "# After creating your plot and before plt.tight_layout(), format y-axis in thousands:\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "ax.set_ylabel(\"Number of Fires (thousands)\")\n",
    "ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _ : f\"{x/1000:.1f}k\"))\n",
    "\n",
    "ax.set_ylim(0, counts.max() * 1.25 if counts.max() > 0 else 1)\n",
    "ax.grid(axis=\"y\", alpha=0.2, linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\FIGURES\\IA_SUCCESS_FIRESIZE_FULL_FED.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 3) By GACCAbbrev (stacked shares and success-rate bars) ----------\n",
    "gacc_col = \"GACCAbbrev\"\n",
    "if gacc_col in df.columns:\n",
    "    dfg = df.dropna(subset=[gacc_col]).copy()\n",
    "\n",
    "    gb = dfg.groupby(gacc_col)[\"event_type\"]\n",
    "    by_counts = gb.value_counts().unstack(fill_value=0).reindex(columns=[0,1], fill_value=0)\n",
    "    by_counts[\"total\"] = by_counts.sum(axis=1)\n",
    "\n",
    "    # Compute shares\n",
    "    by_share = by_counts[[0,1]].div(by_counts[\"total\"].replace(0, np.nan), axis=0).fillna(0)\n",
    "\n",
    "    # Sort by IA success share descending\n",
    "    by_share_sorted = by_share.sort_values(0, ascending=False)\n",
    "    by_counts_sorted = by_counts.loc[by_share_sorted.index]\n",
    "\n",
    "    # --- 3a) Stacked share (IA success vs failure) by GACC ---\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4.5, 0.5*len(by_share_sorted))))\n",
    "    ax.bar(by_share_sorted.index, by_share_sorted[0].values, label=\"IA Success\", color=\"#2ca02c\")\n",
    "    ax.bar(by_share_sorted.index, by_share_sorted[1].values, bottom=by_share_sorted[0].values, label=\"IA Failure\", color=\"#d62728\")\n",
    "    for i, g in enumerate(by_share_sorted.index):\n",
    "        succ_pct = by_share_sorted.loc[g, 0] * 100\n",
    "        ax.text(i, 1.005, f\"{succ_pct:.0f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    #ax.set_title(\"IA Outcomes by GACC\")\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.set_xticks(range(len(by_share_sorted.index)))\n",
    "    ax.set_xticklabels(by_share_sorted.index, rotation=45, ha=\"right\")\n",
    "    ax.legend(loc=\"upper left\", bbox_to_anchor=(1.01, 1.02))\n",
    "    ax.grid(axis=\"y\", alpha=0.2, linestyle=\"--\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\FIGURES\\IA_SUCCESS_FIRESIZE_GACC_FED.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Column 'GACCAbbrev' not found; skipping by-GACC plots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb30af9-8fef-48d8-b87a-4554d17033b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b823d2e0-d7c3-479e-87b5-077aa69841ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c1531-e806-4def-bdaf-38cc29192568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f91be-ecaf-455e-bc47-eafa4b6090e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf000b9-f219-4988-92c8-99011b93b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED 24 hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01ada9-539a-44c7-aa23-be96b4a1dfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4c3f9-ad73-49ec-a146-3641cc515a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Aggregate ----------\n",
    "yearly = (\n",
    "    df.groupby(\"FIRE_YEAR\")[\"event_type\"]\n",
    "      .agg(total=\"count\", large=\"sum\")\n",
    "      .reset_index()\n",
    "      .sort_values(\"FIRE_YEAR\")\n",
    ")\n",
    "yearly = yearly[(yearly[\"FIRE_YEAR\"] >= 1992) & (yearly[\"FIRE_YEAR\"] <= 2020)]\n",
    "yearly[\"small\"] = yearly[\"total\"] - yearly[\"large\"]\n",
    "\n",
    "# ---------- Plot: line chart ----------\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "x = yearly[\"FIRE_YEAR\"].values\n",
    "\n",
    "# Lines with new colors\n",
    "ax.plot(x, yearly[\"small\"].values, marker=\"o\", color=\"#2ca02c\", label=\"Small Fires\")\n",
    "ax.plot(x, yearly[\"large\"].values, marker=\"o\", color=\"#d62728\", label=\"Large Fires\")\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_ylabel(\"Number of Ignitions\", fontsize=12)\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.4, axis=\"y\")\n",
    "\n",
    "# Legend outside to the right\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), fontsize=12, frameon=False)\n",
    "\n",
    "ax.set_ylim(0, max(yearly[\"small\"].max(), yearly[\"large\"].max()) * 1.05)\n",
    "\n",
    "# --- Custom x-axis ticks ---\n",
    "ax.set_xticks([1992, 1995, 2000, 2005, 2010, 2015, 2020])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\FIGURES\\IA_TrendsOverTime_92_2020.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c46fc-0e6c-4081-affa-e329199bf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop NA in ERC\n",
    "df_bin = df.dropna(subset=[\"erc\"]).copy()\n",
    "\n",
    "# Define bins (percentile thresholds)\n",
    "bins = [0, 80, 90, 95, 97.5, 100]\n",
    "labels = [\"0–80\", \"80–90\", \"90–95\", \"95–97.5\", \"97.5–100\"]\n",
    "\n",
    "# Compute percentile cutoffs from the data\n",
    "cutoffs = np.percentile(df_bin[\"erc\"], bins)\n",
    "\n",
    "# Bin ERC values\n",
    "df_bin[\"erc_bin\"] = pd.cut(df_bin[\"erc\"], bins=cutoffs, labels=labels, include_lowest=True)\n",
    "\n",
    "# Count successes (0) vs failures (1) in each bin\n",
    "counts = df_bin.groupby([\"erc_bin\", \"event_type\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Side-by-side bar plot\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "counts.plot(kind=\"bar\", ax=ax, width=0.8, color={0:\"#2ca02c\", 1:\"#d62728\"})\n",
    "\n",
    "ax.set_xlabel(\"ERC Percentile Bin\", fontsize=12)\n",
    "ax.set_ylabel(\"Number of Fires\", fontsize=12)\n",
    "ax.set_title(\"IA Success vs Failure by ERC Percentile Bin\", fontsize=14)\n",
    "ax.legend([\"Success\", \"Failure\"], fontsize=12)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51cfd1-b10a-4649-8086-4f0d337f7b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d2f0e-d7a6-4eae-bb64-370a72f846b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d12ade-beac-4dac-9d0b-384831691571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409ffc9-b7bd-481d-9375-f768af77a55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ead3d9-f745-4a7c-9b50-931c38d33d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop NA in VPD\n",
    "df_bin = df.dropna(subset=[\"vpd\"]).copy()\n",
    "\n",
    "# Define bins (percentile thresholds)\n",
    "bins = [0, 80, 90, 95, 97.5, 100]\n",
    "labels = [\"0–80\", \"80–90\", \"90–95\", \"95–97.5\", \"97.5–100\"]\n",
    "\n",
    "# Compute percentile cutoffs from the data\n",
    "cutoffs = np.percentile(df_bin[\"vpd\"], bins)\n",
    "\n",
    "# Bin VPD values\n",
    "df_bin[\"vpd_bin\"] = pd.cut(df_bin[\"vpd\"], bins=cutoffs, labels=labels, include_lowest=True)\n",
    "\n",
    "# Count successes (0) vs failures (1) in each bin\n",
    "counts = df_bin.groupby([\"vpd_bin\", \"event_type\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Side-by-side bar plot\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "counts.plot(kind=\"bar\", ax=ax, width=0.8, color={0:\"#2ca02c\", 1:\"#d62728\"})\n",
    "\n",
    "ax.set_xlabel(\"VPD Percentile Bin\", fontsize=12)\n",
    "ax.set_ylabel(\"Number of Fires\", fontsize=12)\n",
    "ax.set_title(\"IA Success vs Failure by VPD Percentile Bin\", fontsize=14)\n",
    "ax.legend([\"Success\", \"Failure\"], fontsize=12)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce704af6-de83-4388-bc80-ed8d509aa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme negative SDI fill values with 0\n",
    "df.loc[df[\"SDI\"] < -1e20, \"SDI\"] = 0\n",
    "\n",
    "# Check results\n",
    "print(\"SDI min:\", df[\"SDI\"].min())\n",
    "print(\"SDI max:\", df[\"SDI\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ed12e-50bf-46d6-aee0-8ae061e199c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dfcc6-1151-4741-abcd-ea0394648dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbd7f8-a9a4-4e57-bab1-e477e6f84f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b934392-9b80-4676-a37e-123c60923dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IA success as a function of VPD and ERC: scatter/hexbin + failure-rate heatmap ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Prep: keep rows with both VPD and ERC and event_type in {0,1}\n",
    "# -------------------------------------------------------------------\n",
    "d = df.loc[df[\"vpd\"].notna() & df[\"erc\"].notna() & df[\"event_type\"].isin([0,1]), [\"vpd\",\"erc\",\"event_type\"]].copy()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Hexbin “scatter-like” density, colored by count (two panels)\n",
    "#    Left = IA Success (event_type=0), Right = IA Failure (event_type=1)\n",
    "# -------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "for ax, et, title in zip(\n",
    "    axes,\n",
    "    [0, 1],\n",
    "    [\"IA Success (event_type=0)\", \"IA Failure (event_type=1)\"]\n",
    "):\n",
    "    dd = d[d[\"event_type\"] == et]\n",
    "    hb = ax.hexbin(\n",
    "        dd[\"vpd\"].values, dd[\"erc\"].values,\n",
    "        gridsize=50, mincnt=1, linewidths=0.0, cmap=\"viridis\"\n",
    "    )\n",
    "    ax.set_xlabel(\"VPD\", fontsize=12)\n",
    "    ax.set_ylabel(\"ERC\", fontsize=12)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    cb = fig.colorbar(hb, ax=ax)\n",
    "    cb.set_label(\"Count\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(r\"C:\\path\\to\\FIGURES\\vpd_erc_hexbin_success_failure.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) 2D binned FAILURE RATE heatmap over VPD×ERC\n",
    "#    Color shows share of failures (event_type=1) in each (VPD, ERC) bin\n",
    "# -------------------------------------------------------------------\n",
    "# Define bin edges (adjust ranges as needed)\n",
    "vpd_bins = np.linspace(d[\"vpd\"].quantile(0.01), d[\"vpd\"].quantile(0.99), 30)\n",
    "erc_bins = np.linspace(d[\"erc\"].quantile(0.01), d[\"erc\"].quantile(0.99), 30)\n",
    "\n",
    "# Assign bins\n",
    "vpd_bin_idx = np.digitize(d[\"vpd\"].values, vpd_bins) - 1\n",
    "erc_bin_idx = np.digitize(d[\"erc\"].values, erc_bins) - 1\n",
    "\n",
    "# Keep only indices that fall into valid bin ranges\n",
    "valid_mask = (vpd_bin_idx >= 0) & (vpd_bin_idx < len(vpd_bins)-1) & (erc_bin_idx >= 0) & (erc_bin_idx < len(erc_bins)-1)\n",
    "vb = vpd_bin_idx[valid_mask]\n",
    "eb = erc_bin_idx[valid_mask]\n",
    "evt = d[\"event_type\"].values[valid_mask]\n",
    "\n",
    "# Count totals and failures per cell\n",
    "tot_grid = np.zeros((len(erc_bins)-1, len(vpd_bins)-1), dtype=np.int32)\n",
    "fail_grid = np.zeros_like(tot_grid)\n",
    "np.add.at(tot_grid, (eb, vb), 1)\n",
    "np.add.at(fail_grid, (eb, vb), (evt == 1).astype(int))\n",
    "\n",
    "# Failure rate (avoid divide-by-zero)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    fail_rate = np.where(tot_grid > 0, fail_grid / tot_grid, np.nan)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(7.5, 6))\n",
    "im = ax.imshow(\n",
    "    fail_rate,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    extent=[vpd_bins[0], vpd_bins[-1], erc_bins[0], erc_bins[-1]],\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0.0, vmax=1.0,\n",
    ")\n",
    "ax.set_xlabel(\"VPD\", fontsize=12)\n",
    "ax.set_ylabel(\"ERC\", fontsize=12)\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_title(\"IA Failure Rate by VPD × ERC (share of failures)\", fontsize=12)\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Failure rate (0–1)\", fontsize=12)\n",
    "\n",
    "# Optional: overlay sparse contours of count to show data density\n",
    "# import matplotlib\n",
    "# cs = ax.contour(\n",
    "#     (vpd_bins[:-1][None, :]+vpd_bins[1:][None, :])/2,\n",
    "#     (erc_bins[:-1][:, None]+erc_bins[1:][:, None])/2,\n",
    "#     tot_grid, levels=5, linewidths=0.6, colors=\"k\", alpha=0.4\n",
    "# )\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(r\"C:\\path\\to\\FIGURES\\vpd_erc_failure_rate_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3132ff1-f66f-4c88-a2c7-be9c28b8a32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100735a7-7715-4ab4-8af0-f6d197a81b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd623c-55da-453b-b94a-917e67e8b2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045491ab-d441-4885-921f-40a8d47b2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import PowerNorm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) 2D binned FAILURE RATE heatmap over VPD×ERC\n",
    "#    Color shows share of failures (event_type=1) in each (VPD, ERC) bin\n",
    "# -------------------------------------------------------------------\n",
    "# Define bin edges (adjust ranges as needed)\n",
    "vpd_bins = np.linspace(d[\"vpd\"].quantile(0.01), d[\"vpd\"].quantile(0.99), 75)\n",
    "erc_bins = np.linspace(d[\"erc\"].quantile(0.01), d[\"erc\"].quantile(0.99), 75)\n",
    "\n",
    "# Assign bins\n",
    "vpd_bin_idx = np.digitize(d[\"vpd\"].values, vpd_bins) - 1\n",
    "erc_bin_idx = np.digitize(d[\"erc\"].values, erc_bins) - 1\n",
    "\n",
    "# Keep only indices that fall into valid bin ranges\n",
    "valid_mask = (vpd_bin_idx >= 0) & (vpd_bin_idx < len(vpd_bins)-1) & (erc_bin_idx >= 0) & (erc_bin_idx < len(erc_bins)-1)\n",
    "vb = vpd_bin_idx[valid_mask]\n",
    "eb = erc_bin_idx[valid_mask]\n",
    "evt = d[\"event_type\"].values[valid_mask]\n",
    "\n",
    "# Count totals and failures per cell\n",
    "tot_grid = np.zeros((len(erc_bins)-1, len(vpd_bins)-1), dtype=np.int32)\n",
    "fail_grid = np.zeros_like(tot_grid)\n",
    "np.add.at(tot_grid, (eb, vb), 1)\n",
    "np.add.at(fail_grid, (eb, vb), (evt == 1).astype(int))\n",
    "\n",
    "# Failure rate (avoid divide-by-zero)\n",
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    fail_rate = np.where(tot_grid > 0, fail_grid / tot_grid, np.nan)\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(7.5, 6))\n",
    "im = ax.imshow(\n",
    "    fail_rate,\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    extent=[vpd_bins[0], vpd_bins[-1], erc_bins[0], erc_bins[-1]],\n",
    "    cmap=\"Reds\",\n",
    "    vmin=0.0, vmax=1.0,\n",
    ")\n",
    "ax.set_xlabel(\"VPD\", fontsize=12)\n",
    "ax.set_ylabel(\"ERC\", fontsize=12)\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_title(\"IA Failure Rate by VPD × ERC (share of failures)\", fontsize=12)\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Failure rate (0–1)\", fontsize=12)\n",
    "\n",
    "# Optional: overlay sparse contours of count to show data density\n",
    "# import matplotlib\n",
    "# cs = ax.contour(\n",
    "#     (vpd_bins[:-1][None, :]+vpd_bins[1:][None, :])/2,\n",
    "#     (erc_bins[:-1][:, None]+erc_bins[1:][:, None])/2,\n",
    "#     tot_grid, levels=5, linewidths=0.6, colors=\"k\", alpha=0.4\n",
    "# )\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(r\"C:\\path\\to\\FIGURES\\vpd_erc_failure_rate_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === SDI vs VPD failure-rate heatmap ===\n",
    "d_sdi = df.loc[df[\"SDI\"].notna() & df[\"vpd\"].notna() & df[\"event_type\"].isin([0,1]),\n",
    "               [\"SDI\",\"vpd\",\"event_type\"]].copy()\n",
    "\n",
    "# Define bin edges\n",
    "sdi_bins = np.linspace(d_sdi[\"SDI\"].quantile(0.01), d_sdi[\"SDI\"].quantile(0.99), 75)\n",
    "vpd_bins = np.linspace(d_sdi[\"vpd\"].quantile(0.01), d_sdi[\"vpd\"].quantile(0.99), 75)\n",
    "\n",
    "# Assign bins\n",
    "sdi_idx = np.digitize(d_sdi[\"SDI\"].values, sdi_bins) - 1\n",
    "vpd_idx = np.digitize(d_sdi[\"vpd\"].values, vpd_bins) - 1\n",
    "valid = (sdi_idx >= 0) & (sdi_idx < len(sdi_bins)-1) & (vpd_idx >= 0) & (vpd_idx < len(vpd_bins)-1)\n",
    "\n",
    "# Totals + failures\n",
    "tot = np.zeros((len(sdi_bins)-1, len(vpd_bins)-1), dtype=int)\n",
    "fail = np.zeros_like(tot)\n",
    "np.add.at(tot, (sdi_idx[valid], vpd_idx[valid]), 1)\n",
    "np.add.at(fail, (sdi_idx[valid], vpd_idx[valid]), (d_sdi[\"event_type\"].values[valid] == 1).astype(int))\n",
    "\n",
    "fail_rate = np.where(tot > 0, fail/tot, np.nan)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5,6))\n",
    "im = ax.imshow(fail_rate, origin=\"lower\", aspect=\"auto\",\n",
    "               extent=[vpd_bins[0], vpd_bins[-1], sdi_bins[0], sdi_bins[-1]],\n",
    "               cmap=\"Reds\", vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xlabel(\"VPD\", fontsize=12)\n",
    "ax.set_ylabel(\"SDI\", fontsize=12)\n",
    "ax.set_title(\"IA Failure Rate by SDI × VPD\", fontsize=12)\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Failure rate\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === SDI vs ERC failure-rate heatmap ===\n",
    "d_sdi = df.loc[df[\"SDI\"].notna() & df[\"erc\"].notna() & df[\"event_type\"].isin([0,1]),\n",
    "               [\"SDI\",\"erc\",\"event_type\"]].copy()\n",
    "\n",
    "sdi_bins = np.linspace(d_sdi[\"SDI\"].quantile(0.01), d_sdi[\"SDI\"].quantile(0.99), 75)\n",
    "erc_bins = np.linspace(d_sdi[\"erc\"].quantile(0.01), d_sdi[\"erc\"].quantile(0.99), 75)\n",
    "\n",
    "sdi_idx = np.digitize(d_sdi[\"SDI\"].values, sdi_bins) - 1\n",
    "erc_idx = np.digitize(d_sdi[\"erc\"].values, erc_bins) - 1\n",
    "valid = (sdi_idx >= 0) & (sdi_idx < len(sdi_bins)-1) & (erc_idx >= 0) & (erc_idx < len(erc_bins)-1)\n",
    "\n",
    "tot = np.zeros((len(sdi_bins)-1, len(erc_bins)-1), dtype=int)\n",
    "fail = np.zeros_like(tot)\n",
    "np.add.at(tot, (sdi_idx[valid], erc_idx[valid]), 1)\n",
    "np.add.at(fail, (sdi_idx[valid], erc_idx[valid]), (d_sdi[\"event_type\"].values[valid] == 1).astype(int))\n",
    "\n",
    "fail_rate = np.where(tot > 0, fail/tot, np.nan)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5,6))\n",
    "im = ax.imshow(fail_rate, origin=\"lower\", aspect=\"auto\",\n",
    "               extent=[erc_bins[0], erc_bins[-1], sdi_bins[0], sdi_bins[-1]],\n",
    "               cmap=\"Reds\", vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"ERC\", fontsize=12)\n",
    "ax.set_ylabel(\"SDI\", fontsize=12)\n",
    "ax.set_title(\"IA Failure Rate by SDI × ERC\", fontsize=12)\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Failure rate\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87152c53-fa00-4ff9-9b54-8e384bfc60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many SDI values are at the float32 fill value\n",
    "bad_val = -3.4028234663852885e+36\n",
    "n_bad = (df[\"SDI\"] == bad_val).sum()\n",
    "\n",
    "print(\"Number of SDI entries equal to fill value:\", n_bad)\n",
    "# Count how many SDI values are at the float32 fill value\n",
    "bad_val = -3.4028234663852885e+36\n",
    "n_bad = (df[\"SDI\"] == bad_val).sum()\n",
    "\n",
    "print(\"Number of SDI entries equal to fill value:\", n_bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d37ab8-80bc-4140-a0bb-54660d390e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8e306eff-5172-4363-bb29-38b3bdabb470",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a5710-7a82-45ca-9f09-b5c25c55ccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a66663-86c8-4830-960a-a8ecaafdd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURES\n",
    "df_SUM_Failures = df_SUB[df_SUB[\"event_type\"] == 1].copy()\n",
    "df_SUM_SUCC = df_SUB[df_SUB[\"event_type\"] == 0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4871ffb-3679-4d2f-9dea-576bbdb36f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SUM_Failures connects to For ia failures Connect to 209 =  ics209plus-wf_cpx-assoc-summary_2014to2023-draft (\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_cpx-assoc-summary_2014to2023-draft.csv\")\n",
    "# Not counting complexes probablamitcally\n",
    "# So need to column ‘y’ FOD_ID left join so that ‘e’ CPLX_INCIDENT_ID  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e49c95-87fa-4d6b-a4f3-2c931bdbc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load the ICS-209 summary file ---\n",
    "ics209_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_cpx-assoc-summary_2014to2023-draft.csv\"\n",
    "ics209 = pd.read_csv(ics209_fp)\n",
    "\n",
    "# --- Inspect available columns ---\n",
    "print(ics209.columns)\n",
    "\n",
    "# --- Join with df_SUM_Failures ---\n",
    "# Assume df_SUM_Failures has column \"FOD_ID\"\n",
    "# and ICS-209 file has \"CPLX_INCIDENT_ID\" that should link\n",
    "merged = df_SUM_Failures.merge(\n",
    "    ics209,\n",
    "    how=\"left\",\n",
    "    left_on=\"FOD_ID\",\n",
    "    right_on=\"CPLX_INCIDENT_ID\",\n",
    "    suffixes=(\"\", \"_ics209\")\n",
    ")\n",
    "\n",
    "# --- Drop complex records if needed (to avoid double-counting) ---\n",
    "if \"CPLX_FLG\" in merged.columns:\n",
    "    merged = merged.loc[merged[\"CPLX_FLG\"] != 1]  # adjust flag logic as appropriate\n",
    "\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb297f2-9166-414f-bfba-e39cb7d69e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765667f1-7095-42b1-814f-6f034dd25e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646160b7-a5aa-419f-8d4a-7e040561e42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476dfd1-6689-4397-99c2-9063e3c91692",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load ICS-209\n",
    "ics209_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_cpx-assoc-summary_2014to2023-draft.csv\"\n",
    "ics209 = pd.read_csv(ics209_fp)\n",
    "\n",
    "# Standardize merge keys to string\n",
    "df_SUM_Failures[\"FOD_ID\"] = df_SUM_Failures[\"FOD_ID\"].astype(str)\n",
    "ics209[\"FOD_ID\"] = ics209[\"FOD_ID\"].astype(str)\n",
    "\n",
    "# Merge\n",
    "merged = df_SUM_Failures.merge(\n",
    "    ics209,\n",
    "    how=\"left\",\n",
    "    left_on=\"FOD_ID\",\n",
    "    right_on=\"FOD_ID\",\n",
    "    suffixes=(\"\", \"_ics209\")\n",
    ")\n",
    "\n",
    "print(merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377ebb3-1e5d-439a-9b42-a80d7a8f3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # --- Load ICS-209 files (as you already do) ---\n",
    "# ics209_assoc_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_cpx-assoc-summary_2014to2023-draft.csv\"\n",
    "# ics209_incidents_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_incidents_1999to2023-draft.csv\"\n",
    "\n",
    "# ics209_assoc = pd.read_csv(ics209_assoc_fp)\n",
    "# ics209_incidents = pd.read_csv(ics209_incidents_fp)\n",
    "\n",
    "# # --- Make safe copies ---\n",
    "# df_sf = df_SUM_Failures.copy()\n",
    "# assoc = ics209_assoc.copy()\n",
    "# inc   = ics209_incidents.copy()\n",
    "\n",
    "# # --- Helper: normalize FOD_ID to pure digit strings (removes .0, spaces, non-digits) ---\n",
    "# def normalize_fod(s: pd.Series) -> pd.Series:\n",
    "#     out = s.astype(str).str.strip()\n",
    "#     out = out.str.replace(r'(?i)^nan$', '', regex=True)     # turn 'nan' strings to empty\n",
    "#     out = out.str.replace(r'\\.0+$', '', regex=True)        # drop trailing .0 from floats\n",
    "#     out = out.str.replace(r'\\D', '', regex=True)           # keep only digits\n",
    "#     out = out.where(out.ne(''), np.nan)                    # empty -> NaN\n",
    "#     return out\n",
    "\n",
    "# # --- Normalize FOD_ID on both sides ---\n",
    "# df_sf[\"FOD_ID_norm\"] = normalize_fod(df_sf[\"FOD_ID\"])\n",
    "# assoc[\"FOD_ID_norm\"] = normalize_fod(assoc[\"FOD_ID\"])\n",
    "\n",
    "# # (Optional) keep only 2014+ since ICS-209 assoc is 2014–2023\n",
    "# df_sf_2014p = df_sf.loc[df_sf[\"FIRE_YEAR\"] >= 2014].copy()\n",
    "\n",
    "# # --- Merge failures with assoc on normalized FOD_ID ---\n",
    "# merged = df_sf_2014p.merge(\n",
    "#     assoc.drop(columns=[\"FOD_ID\"], errors=\"ignore\"),\n",
    "#     how=\"left\",\n",
    "#     left_on=\"FOD_ID_norm\",\n",
    "#     right_on=\"FOD_ID_norm\",\n",
    "#     indicator=True,\n",
    "#     suffixes=(\"\", \"_assoc\")\n",
    "# )\n",
    "\n",
    "# # --- Report match rate ---\n",
    "# n_all = len(merged)\n",
    "# n_matched = int((merged[\"_merge\"] == \"both\").sum())\n",
    "# print(f\"Matched on FOD_ID (normalized): {n_matched:,} / {n_all:,} ({n_matched/n_all*100:.1f}%)\")\n",
    "\n",
    "# # --- Standardize & merge incident-level data (assoc 'CPLX_INCIDENT_ID' -> incidents 'INCIDENT_ID') ---\n",
    "# if \"CPLX_INCIDENT_ID\" in assoc.columns and \"INCIDENT_ID\" in inc.columns:\n",
    "#     merged[\"CPLX_INCIDENT_ID_norm\"] = normalize_fod(merged.get(\"CPLX_INCIDENT_ID\"))\n",
    "#     inc[\"INCIDENT_ID_norm\"] = normalize_fod(inc[\"INCIDENT_ID\"])\n",
    "\n",
    "#     merged = merged.merge(\n",
    "#         inc.drop(columns=[\"INCIDENT_ID\"], errors=\"ignore\"),\n",
    "#         how=\"left\",\n",
    "#         left_on=\"CPLX_INCIDENT_ID_norm\",\n",
    "#         right_on=\"INCIDENT_ID_norm\",\n",
    "#         suffixes=(\"\", \"_incident\")\n",
    "#     )\n",
    "\n",
    "# # --- Peek ---\n",
    "# print(merged[[\"FIRE_YEAR\",\"FOD_ID\",\"FOD_ID_norm\",\"_merge\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8cdc1-32a1-47b6-8097-792df296e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # --- Load ICS-209 summary (assoc) ---\n",
    "# ics209_assoc_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_cpx-assoc-summary_2014to2023-draft.csv\"\n",
    "# ics209_assoc = pd.read_csv(ics209_assoc_fp)\n",
    "\n",
    "# # --- Load ICS-209 incidents ---\n",
    "# ics209_incidents_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_incidents_1999to2023-draft.csv\"\n",
    "# ics209_incidents = pd.read_csv(ics209_incidents_fp)\n",
    "\n",
    "# # # --- Standardize join keys ---\n",
    "# # df_SUM_Failures[\"FOD_ID\"] = df_SUM_Failures[\"FOD_ID\"].astype(str)\n",
    "# # ics209_assoc[\"FOD_ID\"] = ics209_assoc[\"FOD_ID\"].astype(str)\n",
    "# # ics209_assoc[\"CPLX_INC_IDENTIFIER\"] = ics209_assoc[\"CPLX_INCIDENT_ID\"].astype(str)\n",
    "# # ics209_incidents[\"CPLX_INC_IDENTIFIER\"] = ics209_incidents[\"INCIDENT_ID\"].astype(str)\n",
    "\n",
    "# # # --- Step 1: Failures + assoc ---\n",
    "# # merged = df_SUM_Failures.merge(\n",
    "# #     ics209_assoc,\n",
    "# #     how=\"left\",\n",
    "# #     on=\"FOD_ID\",\n",
    "# #     suffixes=(\"\", \"_assoc\")\n",
    "# # )\n",
    "\n",
    "# # # --- Step 2: Add incident-level info using CPLX_INCIDENT_ID ---\n",
    "# # merged = merged.merge(\n",
    "# #     ics209_incidents,\n",
    "# #     how=\"left\",\n",
    "# #     on=\"CPLX_INC_IDENTIFIER\",\n",
    "# #     suffixes=(\"\", \"_incident\")\n",
    "# # )\n",
    "\n",
    "# # print(merged.head())\n",
    "# # Clean helpers\n",
    "# def normalize_id(s: pd.Series) -> pd.Series:\n",
    "#     s = s.astype(\"string\")                         # keep real <NA>, not \"nan\"\n",
    "#     s = s.str.strip()\n",
    "#     s = s.str.replace(r'\\.0+$', '', regex=True)    # remove trailing .0\n",
    "#     s = s.str.replace(r'\\D', '', regex=True)       # keep only digits\n",
    "#     return s\n",
    "\n",
    "# # Normalize FOD_ID on both sides\n",
    "# df_SUM_Failures[\"FOD_ID_norm\"] = normalize_id(df_SUM_Failures[\"FOD_ID\"])\n",
    "# ics209_assoc[\"FOD_ID_norm\"]    = normalize_id(ics209_assoc[\"FOD_ID\"])\n",
    "\n",
    "# # Normalize complex/incident IDs, too\n",
    "# ics209_assoc[\"CPLX_INCIDENT_ID_norm\"] = normalize_id(ics209_assoc[\"CPLX_INCIDENT_ID\"])\n",
    "# ics209_incidents[\"INCIDENT_ID_norm\"]  = normalize_id(ics209_incidents[\"INCIDENT_ID\"])\n",
    "\n",
    "# # --- Step 1: Failures + assoc on normalized FOD_ID ---\n",
    "# merged = df_SUM_Failures.merge(\n",
    "#     ics209_assoc,\n",
    "#     how=\"left\",\n",
    "#     left_on=\"FOD_ID_norm\",\n",
    "#     right_on=\"FOD_ID_norm\",\n",
    "#     suffixes=(\"\", \"_assoc\"),\n",
    "#     indicator=True\n",
    "# )\n",
    "\n",
    "# # # --- Step 2: Add incident-level info using normalized IDs ---\n",
    "# # merged = merged.merge(\n",
    "# #     ics209_incidents,\n",
    "# #     how=\"left\",\n",
    "# #     left_on=\"CPLX_INCIDENT_ID_norm\",\n",
    "# #     right_on=\"INCIDENT_ID_norm\",\n",
    "# #     suffixes=(\"\", \"_incident\")\n",
    "# # )\n",
    "\n",
    "# # Make sure FIRE_SIZE is numeric\n",
    "# merged[\"FIRE_SIZE_num\"] = pd.to_numeric(merged[\"FIRE_SIZE\"], errors=\"coerce\")\n",
    "\n",
    "# # --- First: merge on CPLX_INCIDENT_ID_norm ---\n",
    "# m1 = merged.merge(\n",
    "#     ics209_incidents,\n",
    "#     how=\"left\",\n",
    "#     left_on=\"CPLX_INCIDENT_ID_norm\",\n",
    "#     right_on=\"INCIDENT_ID_norm\",\n",
    "#     suffixes=(\"\", \"_incident\")\n",
    "# )\n",
    "\n",
    "# # --- Identify rows that did NOT merge on complex id ---\n",
    "# no_match_mask = m1[\"INCIDENT_ID_norm\"].isna()\n",
    "\n",
    "# # --- Second: merge those rows on FOD_ID_norm instead ---\n",
    "# m2 = merged.loc[no_match_mask].merge(\n",
    "#     ics209_incidents,\n",
    "#     how=\"left\",\n",
    "#     left_on=\"FOD_ID_norm\",\n",
    "#     right_on=\"FOD_ID_norm\",   # need to normalize FOD_ID in ics209_incidents too\n",
    "#     suffixes=(\"\", \"_incident\")\n",
    "# )\n",
    "\n",
    "# # --- Combine back together ---\n",
    "# merged_final = pd.concat([m1.loc[~no_match_mask], m2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40b059-f2a3-4701-9862-ae4b67139e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de44bcc-7b43-4765-a5e9-2592864e5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load ICS-209 summary (assoc) ---\n",
    "ics209_assoc_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_cpx-assoc-summary_2014to2023-draft.csv\"\n",
    "#ics209_assoc = pd.read_csv(ics209_assoc_fp)\n",
    "\n",
    "# --- Load ICS-209 incidents ---\n",
    "ics209_incidents_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\ics209plus-wf_incidents_1999to2023-draft.csv\"\n",
    "#ics209_incidents = pd.read_csv(ics209_incidents_fp)\n",
    "\n",
    "\n",
    "# Helper\n",
    "def normalize_id(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\").str.strip()\n",
    "    s = s.str.replace(r'\\.0+$', '', regex=True)    # drop trailing .0\n",
    "    s = s.str.replace(r'\\D', '', regex=True)       # keep only digits\n",
    "    return s\n",
    "\n",
    "# Load\n",
    "ics209_assoc = pd.read_csv(ics209_assoc_fp, low_memory=False)\n",
    "ics209_incidents = pd.read_csv(ics209_incidents_fp, low_memory=False)\n",
    "\n",
    "# Normalize keys\n",
    "df_SUM_Failures[\"FOD_ID_norm\"] = normalize_id(df_SUM_Failures[\"FOD_ID\"])\n",
    "ics209_assoc[\"FOD_ID_norm\"]    = normalize_id(ics209_assoc[\"FOD_ID\"])\n",
    "ics209_assoc[\"CPLX_INCIDENT_ID_norm\"] = normalize_id(ics209_assoc[\"CPLX_INCIDENT_ID\"])\n",
    "ics209_incidents[\"INCIDENT_ID_norm\"]  = normalize_id(ics209_incidents[\"INCIDENT_ID\"])\n",
    "if \"FOD_ID\" in ics209_incidents.columns:\n",
    "    ics209_incidents[\"FOD_ID_norm\"] = normalize_id(ics209_incidents[\"FOD_ID\"])\n",
    "else:\n",
    "    ics209_incidents[\"FOD_ID_norm\"] = pd.Series(pd.NA, index=ics209_incidents.index)\n",
    "\n",
    "# Merge failures with assoc on FOD_ID_norm\n",
    "merged = df_SUM_Failures.merge(\n",
    "    ics209_assoc,\n",
    "    how=\"left\",\n",
    "    on=\"FOD_ID_norm\",\n",
    "    suffixes=(\"\", \"_assoc\"),\n",
    "    indicator=False\n",
    ")\n",
    "\n",
    "# First try: join incidents on complex id\n",
    "m1 = merged.merge(\n",
    "    ics209_incidents,\n",
    "    how=\"left\",\n",
    "    left_on=\"CPLX_INCIDENT_ID_norm\",\n",
    "    right_on=\"INCIDENT_ID_norm\",\n",
    "    suffixes=(\"\", \"_incident\")\n",
    ")\n",
    "\n",
    "# Rows that did NOT match on complex id\n",
    "no_match_mask = m1[\"INCIDENT_ID_norm\"].isna()\n",
    "\n",
    "# Fallback: those rows, join on FOD_ID_norm instead\n",
    "base_cols = merged.columns  # columns before incident join\n",
    "fallback_base = merged.loc[no_match_mask, base_cols].copy()\n",
    "m2 = fallback_base.merge(\n",
    "    ics209_incidents,\n",
    "    how=\"left\",\n",
    "    left_on=\"FOD_ID_norm\",\n",
    "    right_on=\"FOD_ID_norm\",\n",
    "    suffixes=(\"\", \"_incident\")\n",
    ")\n",
    "\n",
    "# Combine: complex-id matches + fallback FOD_ID matches\n",
    "merged = pd.concat([m1.loc[~no_match_mask], m2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d2947-ad78-400d-890e-0c0d5b27236b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078de0ca-c4a0-4155-ab32-d947e40b1a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba43c45-f7a3-4e74-b7f9-5c7917eac037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158fe3f-f9fd-41e8-8716-17945face778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68dcc79-16ca-42ac-80a9-259613627cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41638ce-8098-4080-8530-583d0f39a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Sanity: how many complexes repeat?\n",
    "dup_counts = merged[\"CPLX_INCIDENT_ID_norm\"].value_counts(dropna=True)\n",
    "print(\"Complex IDs with >1 rows:\", (dup_counts > 1).sum())\n",
    "\n",
    "# 1) Ensure FIRE_SIZE is numeric\n",
    "merged[\"FIRE_SIZE_num\"] = pd.to_numeric(merged[\"FIRE_SIZE\"], errors=\"coerce\")\n",
    "\n",
    "# 2) Rows WITH a complex: keep the row with the max FIRE_SIZE per complex\n",
    "#    (idxmax is robust and fast)\n",
    "idx_max = (\n",
    "    merged.loc[merged[\"CPLX_INCIDENT_ID_norm\"].notna()]\n",
    "          .groupby(\"CPLX_INCIDENT_ID_norm\")[\"FIRE_SIZE_num\"]\n",
    "          .idxmax()\n",
    ")\n",
    "\n",
    "with_complex = merged.loc[idx_max]\n",
    "\n",
    "# 3) Rows WITHOUT a complex: keep all\n",
    "no_complex = merged.loc[merged[\"CPLX_INCIDENT_ID_norm\"].isna()]\n",
    "\n",
    "# 4) Combine back\n",
    "merged_final = pd.concat([with_complex, no_complex], ignore_index=True)\n",
    "\n",
    "# Optional: report reduction\n",
    "print(f\"Before: {len(merged):,}  After: {len(merged_final):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653703f1-82ce-42d0-9b5e-7cdcfbd0417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a837e-7a71-4a47-8fff-7e61ed9c2f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1a3c2-7572-4e72-aada-e22b8a324aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merged_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8be9a3-6617-425c-9251-dd1a076391d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956e218-5d9a-4586-9044-6050e4c11b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c96b3-6963-400a-b1dc-a5a559830c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c1175-0891-4a51-ab70-adc75168132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_SUM_Failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31a522-a132-4ea7-94d5-9b267fbf98c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e53d32-f2c3-48d9-ae80-aeb5d67fb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_dir = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_fp = os.path.join(out_dir, \"merged_failures_ics209.csv\")\n",
    "\n",
    "merged_final.to_csv(out_fp, index=False)\n",
    "print(f\"Wrote CSV to: {out_fp}\")\n",
    "print(f\"Rows: {len(merged_final):,} | Columns: {merged_final.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c73921-bd96-4492-be18-062358e95bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_DAMAGED_TOTAL, TOTAL_PERSONNEL_SUM, PROJECTED_FINAL_IM_COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cb1fc-a6bb-4ddb-8866-638d5012fe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787a65c-8eec-4aa2-a515-9d8e6924952b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b81f43-230d-4729-9a62-c5b8736d092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Make sure the column is numeric\n",
    "# merged_final[\"PROJECTED_FINAL_IM_COST\"] = pd.to_numeric(\n",
    "#     merged_final[\"PROJECTED_FINAL_IM_COST\"], errors=\"coerce\"\n",
    "# )\n",
    "\n",
    "# # Group by FIRE_YEAR and sum\n",
    "# cost_by_year = (\n",
    "#     merged_final.groupby(\"FIRE_YEAR\")[\"PROJECTED_FINAL_IM_COST\"]\n",
    "#     .sum(min_count=1)   # keep NaN if all values are missing\n",
    "#     .reset_index()\n",
    "#     .sort_values(\"FIRE_YEAR\")\n",
    "# )\n",
    "\n",
    "# # Plot\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.plot(cost_by_year[\"FIRE_YEAR\"], cost_by_year[\"PROJECTED_FINAL_IM_COST\"],\n",
    "#         marker=\"o\", color=\"firebrick\", linewidth=2)\n",
    "\n",
    "# ax.set_xlabel(\"Year\", fontsize=12)\n",
    "# ax.set_ylabel(\"Projected Final IM Cost (sum)\", fontsize=12)\n",
    "# ax.set_title(\"Projected Final IM Cost by Year\", fontsize=14)\n",
    "# ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0cc9b-4061-4c40-afa2-e48ef15fd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the column is numeric\n",
    "merged_final[\"PROJECTED_FINAL_IM_COST\"] = pd.to_numeric(\n",
    "    merged_final[\"PROJECTED_FINAL_IM_COST\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Group by FIRE_YEAR and sum (convert to millions)\n",
    "cost_by_year = (\n",
    "    merged_final.groupby(\"FIRE_YEAR\")[\"PROJECTED_FINAL_IM_COST\"]\n",
    "    .sum(min_count=1)   # keep NaN if all values are missing\n",
    "    .div(1e9)           # convert to millions\n",
    "    .reset_index()\n",
    "    .sort_values(\"FIRE_YEAR\")\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(cost_by_year[\"FIRE_YEAR\"], cost_by_year[\"PROJECTED_FINAL_IM_COST\"],\n",
    "        marker=\"o\", color=\"firebrick\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.set_ylabel(\"Projected Final IM Cost (Billion $)\", fontsize=12)\n",
    "#ax.set_title(\"Projected Final IM Cost by Year\", fontsize=14)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30a348-9355-4ca2-a213-b74898c035a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b8f83-de6e-49ac-b2d1-3a16689883df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc4425-6611-48e7-94bb-f1afc2c4a544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a7e01-7aa0-4223-b48e-e0630b8bf378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the column is numeric\n",
    "merged_final[\"STR_DAMAGED_TOTAL\"] = pd.to_numeric(\n",
    "    merged_final[\"STR_DAMAGED_TOTAL\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Group by FIRE_YEAR and sum (convert to thousands if needed, here kept as raw counts)\n",
    "damaged_by_year = (\n",
    "    merged_final.groupby(\"FIRE_YEAR\")[\"STR_DAMAGED_TOTAL\"]\n",
    "    .sum(min_count=1)   \n",
    "    .reset_index()\n",
    "    .sort_values(\"FIRE_YEAR\")\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(damaged_by_year[\"FIRE_YEAR\"], damaged_by_year[\"STR_DAMAGED_TOTAL\"],\n",
    "        marker=\"o\", color=\"firebrick\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.set_ylabel(\"Structures Damaged (count)\", fontsize=12)\n",
    "#ax.set_title(\"Structures Damaged by Year\", fontsize=14)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4c33d-eda7-43b0-ad8a-c69e8c57dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = [\n",
    "    \"STR_DAMAGED_COMM_TOTAL\",\n",
    "    \"STR_DAMAGED_RES_TOTAL\",\n",
    "    \"STR_DAMAGED_TOTAL\",\n",
    "    \"STR_DESTROYED_COMM_TOTAL\",\n",
    "    \"STR_DESTROYED_RES_TOTAL\",\n",
    "    \"STR_DESTROYED_TOTAL\",\n",
    "    \"STR_THREATENED_COMM_MAX\",\n",
    "    \"STR_THREATENED_MAX\",\n",
    "    \"STR_THREATENED_RES_MAX\"\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    # Make sure numeric\n",
    "    merged_final[col] = pd.to_numeric(merged_final[col], errors=\"coerce\")\n",
    "\n",
    "    # Aggregate by year\n",
    "    yearly = (\n",
    "        merged_final.groupby(\"FIRE_YEAR\")[col]\n",
    "        .sum(min_count=1)\n",
    "        .reset_index()\n",
    "        .sort_values(\"FIRE_YEAR\")\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.plot(yearly[\"FIRE_YEAR\"], yearly[col],\n",
    "            marker=\"o\", color=\"firebrick\", linewidth=2)\n",
    "\n",
    "    ax.set_xlabel(\"Year\", fontsize=12)\n",
    "    ax.set_ylabel(f\"{col} (count)\", fontsize=12)\n",
    "    ax.set_title(f\"{col} by Year\", fontsize=14)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d4d63-0024-4e68-b3b8-077ce63e470a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bd59f-2009-42ad-bd90-e625cfc6905d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b8e88-971c-4f30-a4cc-cadd9afa9a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b61efc-2e68-4dfc-a6ff-b3acb9dd94de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c154d59-97ab-42b6-968c-6f5bd6247af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Make sure the column is numeric\n",
    "# merged_final[\"TOTAL_PERSONNEL_SUM\"] = pd.to_numeric(\n",
    "#     merged_final[\"TOTAL_PERSONNEL_SUM\"], errors=\"coerce\"\n",
    "# )\n",
    "\n",
    "# # Group by FIRE_YEAR and sum\n",
    "# personnel_by_year = (\n",
    "#     merged_final.groupby(\"FIRE_YEAR\")[\"TOTAL_PERSONNEL_SUM\"]\n",
    "#     .sum(min_count=1)   \n",
    "#     .reset_index()\n",
    "#     .sort_values(\"FIRE_YEAR\")\n",
    "# )\n",
    "\n",
    "# # Plot\n",
    "# fig, ax = plt.subplots(figsize=(10,6))\n",
    "# ax.plot(personnel_by_year[\"FIRE_YEAR\"], personnel_by_year[\"TOTAL_PERSONNEL_SUM\"],\n",
    "#         marker=\"o\", color=\"firebrick\", linewidth=2)\n",
    "\n",
    "# ax.set_xlabel(\"Year\", fontsize=12)\n",
    "# ax.set_ylabel(\"Total Personnel (sum)\", fontsize=12)\n",
    "# #ax.set_title(\"Total Personnel by Year\", fontsize=14)\n",
    "# ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the column is numeric\n",
    "merged_final[\"TOTAL_PERSONNEL_SUM\"] = pd.to_numeric(\n",
    "    merged_final[\"TOTAL_PERSONNEL_SUM\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Group by FIRE_YEAR and sum (convert to hundred-thousands)\n",
    "personnel_by_year = (\n",
    "    merged_final.groupby(\"FIRE_YEAR\")[\"TOTAL_PERSONNEL_SUM\"]\n",
    "    .sum(min_count=1)\n",
    "    .div(1e6)   # convert to units of 100,000\n",
    "    .reset_index()\n",
    "    .sort_values(\"FIRE_YEAR\")\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(personnel_by_year[\"FIRE_YEAR\"], personnel_by_year[\"TOTAL_PERSONNEL_SUM\"],\n",
    "        marker=\"o\", color=\"firebrick\", linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Year\", fontsize=12)\n",
    "ax.set_ylabel(\"Total Personnel Days (in millions)\", fontsize=12)\n",
    "#ax.set_title(\"Total Personnel by Year\", fontsize=14)\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556027dd-ed24-4e95-af59-ed1b91d07647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I put this above\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # --- Start from merged_final and ensure no sjoin-reserved columns are present ---\n",
    "# df = merged_final.copy()\n",
    "# for col in [\"index_right\", \"index_left\"]:\n",
    "#     if col in df.columns:\n",
    "#         df = df.drop(columns=col)\n",
    "\n",
    "# # --- Keep only rows with valid coords ---\n",
    "# mask_xy = df[\"LATITUDE\"].notna() & df[\"LONGITUDE\"].notna()\n",
    "# df_xy = df.loc[mask_xy].copy()\n",
    "\n",
    "# # --- Build point GeoDataFrame ---\n",
    "# gdf_points = gpd.GeoDataFrame(\n",
    "#     df_xy,\n",
    "#     geometry=gpd.points_from_xy(df_xy[\"LONGITUDE\"], df_xy[\"LATITUDE\"]),\n",
    "#     crs=\"EPSG:4326\"\n",
    "# )\n",
    "\n",
    "# # --- Load GACC polygons ---\n",
    "# gacc_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\National_GACC_Final_20250113.shp\"\n",
    "# gdf_gacc = gpd.read_file(gacc_fp)[[\"GACCAbbrev\", \"geometry\"]].reset_index(drop=True)\n",
    "\n",
    "# # --- Reproject if needed ---\n",
    "# if gdf_gacc.crs != gdf_points.crs:\n",
    "#     gdf_gacc = gdf_gacc.to_crs(gdf_points.crs)\n",
    "\n",
    "# # --- Spatial join (avoid 'index_right' collision via suffixes) ---\n",
    "# gdf_joined = gpd.sjoin(\n",
    "#     gdf_points.reset_index(),               # keep original index to map back\n",
    "#     gdf_gacc,\n",
    "#     how=\"left\",\n",
    "#     predicate=\"within\",\n",
    "#     lsuffix=\"PTS\",\n",
    "#     rsuffix=\"GACC\"\n",
    "# )\n",
    "\n",
    "# # --- Extract GACCAbbrev as GACCTRUTH and map back to full dataframe ---\n",
    "# gacc_map = gdf_joined.set_index(\"index\")[\"GACCAbbrev\"]\n",
    "# df[\"GACCTRUTH\"] = np.nan\n",
    "# df.loc[gacc_map.index, \"GACCTRUTH\"] = gacc_map.values\n",
    "\n",
    "# # --- Put result back into merged_final ---\n",
    "# merged_final = df\n",
    "\n",
    "# # Optional quick check:\n",
    "# # print(merged_final[\"GACCTRUTH\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bba46a-3a95-44ba-861f-c46ff810a1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730ee5a-7b8e-4fb8-9809-77433661260d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713bae73-db39-48a6-80a8-0db696badad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Start from merged_final and ensure no sjoin-reserved columns are present ---\n",
    "df = merged_final.copy()\n",
    "for col in [\"index_right\", \"index_left\"]:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=col)\n",
    "\n",
    "# --- Keep only rows with valid coords ---\n",
    "mask_xy = df[\"LATITUDE\"].notna() & df[\"LONGITUDE\"].notna()\n",
    "df_xy = df.loc[mask_xy].copy()\n",
    "\n",
    "# --- Build point GeoDataFrame ---\n",
    "gdf_points = gpd.GeoDataFrame(\n",
    "    df_xy,\n",
    "    geometry=gpd.points_from_xy(df_xy[\"LONGITUDE\"], df_xy[\"LATITUDE\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# --- Load GACC polygons and rename the abbrev column to a unique name ---\n",
    "gacc_fp = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Data\\National_GACC_Final_20250113.shp\"\n",
    "gdf_gacc_raw = gpd.read_file(gacc_fp)\n",
    "\n",
    "# sanity check: ensure the field exists (it does on your file, but this guards future runs)\n",
    "if \"GACCAbbrev\" not in gdf_gacc_raw.columns:\n",
    "    raise KeyError(f\"'GACCAbbrev' not found in {gacc_fp}. Available columns: {list(gdf_gacc_raw.columns)}\")\n",
    "\n",
    "gdf_gacc = gdf_gacc_raw[[\"GACCAbbrev\", \"geometry\"]].reset_index(drop=True).rename(\n",
    "    columns={\"GACCAbbrev\": \"GACCTRUTH\"}\n",
    ")\n",
    "\n",
    "# --- Reproject if needed ---\n",
    "if gdf_gacc.crs != gdf_points.crs:\n",
    "    gdf_gacc = gdf_gacc.to_crs(gdf_points.crs)\n",
    "\n",
    "# --- Spatial join (no suffixes needed now that names are unique) ---\n",
    "gdf_joined = gpd.sjoin(\n",
    "    gdf_points.reset_index(),   # keep original index to map back\n",
    "    gdf_gacc,\n",
    "    how=\"left\",\n",
    "    predicate=\"within\",\n",
    ")\n",
    "\n",
    "# figure out the left index column name robustly (usually 'index')\n",
    "left_idx_col = \"index\" if \"index\" in gdf_joined.columns else next(\n",
    "    c for c in gdf_joined.columns if c.startswith(\"index\")\n",
    ")\n",
    "\n",
    "# --- Extract GACCTRUTH and map back to full dataframe ---\n",
    "gacc_map = gdf_joined.set_index(left_idx_col)[\"GACCTRUTH\"]\n",
    "df[\"GACCTRUTH\"] = np.nan\n",
    "df.loc[gacc_map.index, \"GACCTRUTH\"] = gacc_map.values\n",
    "\n",
    "# --- Put result back into merged_final ---\n",
    "merged_final = df\n",
    "\n",
    "# Optional quick check:\n",
    "# print(merged_final[\"GACCTRUTH\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbea2d6-683c-4be5-ab9f-60d10eac46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all unique values (including NaN)\n",
    "print(\"Value counts for GACCTRUTH (including NaN):\")\n",
    "print(merged_final[\"GACCTRUTH\"].value_counts(dropna=False))\n",
    "\n",
    "# If you just want the unique list and how many\n",
    "unique_vals = merged_final[\"GACCTRUTH\"].unique()\n",
    "print(\"\\nUnique values:\", unique_vals)\n",
    "print(\"Number of unique values:\", merged_final[\"GACCTRUTH\"].nunique(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce03b05-2cee-42d5-881e-e51d646626eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61398dc-efa2-456f-84e6-379644ec13bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12780b81-663b-489f-bddf-959ba9676ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Start from your merged_final\n",
    "df_SEASONS = merged_final.copy()\n",
    "\n",
    "# --- Robust DISCOVERY_DATE parsing (handles strings and Excel-serial numbers) ---\n",
    "disc = pd.to_datetime(df_SEASONS[\"DISCOVERY_DATE\"], errors=\"coerce\")\n",
    "if disc.isna().mean() > 0.9 and pd.api.types.is_numeric_dtype(df_SEASONS[\"DISCOVERY_DATE\"]):\n",
    "    disc = pd.to_datetime(df_SEASONS[\"DISCOVERY_DATE\"], unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "df_SEASONS[\"_DISC_DT\"] = disc\n",
    "\n",
    "# --- Map EXACT GACCAbbrev values you have to canonical keys we’ll use in the window table ---\n",
    "# Unique values you reported: ONCC, GBCC, NWCC, SWCC, NRCC, AICC, SACC, RMCC, OSCC, EACC, NaN\n",
    "def map_gacc_exact(x):\n",
    "    if pd.isna(x):\n",
    "        return \"UNK\"\n",
    "    t = str(x).strip().upper()\n",
    "    mapping = {\n",
    "        \"AICC\": \"AICC\",   # Alaska\n",
    "        \"NWCC\": \"NWCC\",   # Northwest\n",
    "        \"NRCC\": \"NRCC\",   # Northern Rockies\n",
    "        \"RMCC\": \"RMCC\",   # Rocky Mountain\n",
    "        \"GBCC\": \"GBCC\",   # Great Basin\n",
    "        \"SWCC\": \"SWCC\",   # Southwest\n",
    "        \"ONCC\": \"ONCC\",   # North Ops (CA-N)\n",
    "        \"OSCC\": \"OSCC\",   # South Ops (CA-S)\n",
    "        \"EACC\": \"EACC\",   # Eastern (Great Lakes & Northeast)\n",
    "        \"SACC\": \"SACC\",   # Southern (Southeast)\n",
    "    }\n",
    "    return mapping.get(t, \"UNK\")\n",
    "\n",
    "df_SEASONS[\"_GACC_KEY\"] = df_SEASONS[\"GACCTRUTH\"].map(map_gacc_exact)\n",
    "\n",
    "# --- Season windows keyed by those exact abbreviations ---\n",
    "# (month, day) inclusive windows derived from your map\n",
    "SEASON_WINDOWS = {\n",
    "    \"AICC\": [(5, 1, 8, 31)],                       # Alaska: May–Aug\n",
    "    \"NWCC\": [(6, 1, 10, 31)],                      # Northwest: Jun–Oct\n",
    "    \"NRCC\": [(6, 1, 9, 30)],                       # Northern Rockies: Jun–Sep\n",
    "    \"RMCC\": [(6, 1, 9, 30)],                       # Rocky Mountain: Jun–Sep\n",
    "    \"GBCC\": [(6, 1, 9, 30)],                       # Great Basin: Jun–Sep\n",
    "    \"SWCC\": [(5, 1, 7, 15)],                       # Southwest: May–mid July\n",
    "    \"ONCC\": [(5, 1, 10, 31)],                      # North CA (North Ops): May–Oct\n",
    "    \"OSCC\": [(5, 1, 10, 31)],                      # South CA (South Ops): May–Oct\n",
    "    \"EACC\": [(3, 1, 5, 31), (10, 1, 11, 30)],      # Great Lakes & Northeast: Mar–May, Oct–Nov\n",
    "    \"SACC\": [(2, 1, 5, 31), (10, 1, 11, 30)],      # Southeast: Feb–May, Oct–Nov\n",
    "    \"UNK\":  []                                      # Unknown/NaN -> treat as out of season\n",
    "}\n",
    "\n",
    "# --- Helper: safe window check ---\n",
    "def in_windows(dt, windows):\n",
    "    if pd.isna(dt) or not windows:\n",
    "        return False\n",
    "    m, d = dt.month, dt.day\n",
    "    for sm, sd, em, ed in windows:\n",
    "        if (sm, sd) <= (em, ed):  # same-year window\n",
    "            if (m, d) >= (sm, sd) and (m, d) <= (em, ed):\n",
    "                return True\n",
    "        else:  # wrap-around window (not used here)\n",
    "            if (m, d) >= (sm, sd) or (m, d) <= (em, ed):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Map each row's GACC to its windows; default to [] for unknowns/NaN\n",
    "gacc_windows = df_SEASONS[\"_GACC_KEY\"].map(SEASON_WINDOWS)\n",
    "\n",
    "# Compute SEASON flag (1 in-season, 0 out-of-season)\n",
    "df_SEASONS[\"SEASON\"] = np.fromiter(\n",
    "    (in_windows(dt, wins) for dt, wins in zip(df_SEASONS[\"_DISC_DT\"], gacc_windows)),\n",
    "    dtype=bool\n",
    ").astype(\"int8\")\n",
    "\n",
    "# Cleanup helpers\n",
    "df_SEASONS.drop(columns=[\"_DISC_DT\", \"_GACC_KEY\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4ce7b-6001-4768-91bc-0815b264cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SEASONS\n",
    "\n",
    "import os\n",
    "\n",
    "out_dir = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\IA_CONTAINMENT\\Outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_fp = os.path.join(out_dir, \"merged_failures_ics209_SEASONS.csv\")\n",
    "\n",
    "df_SEASONS.to_csv(out_fp, index=False)\n",
    "print(f\"Wrote CSV to: {out_fp}\")\n",
    "print(f\"Rows: {len(merged_final):,} | Columns: {merged_final.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db77c5f-046e-4d0d-9505-118e4b3bb952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d20a2-1496-406a-9cf2-d01de6d64b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d59b9-45ad-4986-a95f-5c002ce98600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Aggregate counts by FIRE_YEAR and SEASON ---\n",
    "season_counts = (\n",
    "    df_SEASONS.groupby([\"FIRE_YEAR\", \"SEASON\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .pivot(index=\"FIRE_YEAR\", columns=\"SEASON\", values=\"count\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# Ensure both 0 and 1 columns exist\n",
    "for col in [0, 1]:\n",
    "    if col not in season_counts.columns:\n",
    "        season_counts[col] = 0\n",
    "\n",
    "# --- Plot grouped (side-by-side) bar chart ---\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "season_counts[[0, 1]].plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=False,\n",
    "    ax=ax,\n",
    "    width=0.8,\n",
    "    color={0: \"gray\", 1: \"firebrick\"}\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Fire Year\", fontsize=12)\n",
    "ax.set_ylabel(\"Number of Fires\", fontsize=12)\n",
    "ax.set_title(\"Fires In-Season vs Out-of-Season by Year\", fontsize=14, weight=\"bold\")\n",
    "ax.legend([\"Out of Season (0)\", \"In Season (1)\"], title=\"Season\", loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662d15a-fee3-4d4b-ab77-d2206b5a9ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d9aed-1245-459b-8537-852d1c2264e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdaa92a-56fe-4446-8b61-e6149524a200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2065d-4ade-4265-8613-1aeaf5c6fbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdb380-6370-4927-bab7-9813f4db894d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c9d74-9a7c-43d6-8eae-f27957b0c1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dd01e-6ad5-4264-80d1-4fd24b6bec5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db45dc4-5cc5-4e2b-9138-8f8bb3a36304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb316481-0690-4a0d-9a5d-6a80ef41b834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
