{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018b6df-3697-4ab0-9cfd-aa266e7196e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9279297-9432-462c-9555-d5b65ad3e442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\AppData\\Local\\Temp\\ipykernel_5560\\3623525150.py:11: DtypeWarning: Columns (14,15,16,17,311) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"D:\\Aviation\\IMSR\\final_df_resource_summary_AND_gacc_activity_AND_national_activity_2015_2021_WITH_water_lines_cooridors.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455960\n",
      "Cleaned matched_clean length: 445107\n",
      "Event type counts:\n",
      "event_type\n",
      "0    435433\n",
      "1      9674\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of unique event types: 2\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV\n",
    "# df = pd.read_csv(r\"D:\\Aviation\\IMSR\\final_df_resource_summary_AND_gacc_activity_AND_national_activity_2015_2021_WITH_water_lines_cooridors.csv\")\n",
    "\n",
    "# # Print column names\n",
    "# print(df.columns.tolist())\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(r\"D:\\Aviation\\IMSR\\final_df_resource_summary_AND_gacc_activity_AND_national_activity_2015_2021_WITH_water_lines_cooridors.csv\")\n",
    "\n",
    "# Print column names\n",
    "#print(df.columns.tolist())\n",
    "\n",
    "df = df[\n",
    "    (df['RPL_THEMES'] != -999) &\n",
    "    (df['EPL_MUNIT'] != -999)\n",
    "]\n",
    "\n",
    "mapping = {'LIGHT': 0, 'MODERATE': 1, 'HEAVY': 2}\n",
    "df['initial_attack_activity'] = df['initial_attack_activity'].map(mapping)\n",
    "\n",
    "\n",
    "\n",
    "matched = df\n",
    "\n",
    "import numpy as np\n",
    "# Define variable sets\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal', 'fm100_Normal',\n",
    "    'srad_Normal', 'sph_Normal', 'Slope', 'TRI', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + values_vars + capacity_vars\n",
    "\n",
    "# # Define placeholder values and problematic columns\n",
    "# placeholder_vals = [-999, -999.0, -3.402823e+38]\n",
    "# problem_columns = ['SDI', 'GHM', 'RPL_THEMES', 'EPL_MUNIT']\n",
    "\n",
    "# # Print length before cleaning\n",
    "# print(f\"Original matched length: {len(matched)}\")\n",
    "\n",
    "# # Replace known placeholder values with NaN\n",
    "# for col in problem_columns:\n",
    "#     matched[col] = matched[col].replace(placeholder_vals, np.nan)\n",
    "\n",
    "print(len(matched))\n",
    "\n",
    "# Drop rows where SDI or GHM are < 0\n",
    "#matched = matched[(matched['SDI'] > 0) & (matched['GHM'] > 0)]\n",
    "\n",
    "matched['SDI'] = matched['SDI'].where(matched['SDI'] >= 0, 0)\n",
    "matched['GHM'] = matched['GHM'].where(matched['GHM'] >= 0, 0)\n",
    "\n",
    "# Drop rows with missing predictor or target values\n",
    "matched_clean = matched.dropna(subset=predictors + ['event_type'])\n",
    "\n",
    "# Print length after cleaning\n",
    "print(f\"Cleaned matched_clean length: {len(matched_clean)}\")\n",
    "\n",
    "# Print unique event_type values and their counts\n",
    "event_counts = matched_clean['event_type'].value_counts()\n",
    "print(\"Event type counts:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print number of unique event types\n",
    "print(f\"\\nNumber of unique event types: {matched_clean['event_type'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7cff0-13cd-4e26-86ee-e9e4c7e5dfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f2fe0-acb7-4833-bfad-b5c924407fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define predictor sets\n",
    "# hazard_vars = [\n",
    "#     'vpd_Normal', 'bi_Normal', 'fm100_Normal', 'srad_Normal', 'sph_Normal',\n",
    "#     'Slope', 'TRI', 'TPI', 'Elevation', 'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "# ]\n",
    "# values_vars = ['Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT']\n",
    "# capacity_vars = [\n",
    "#     'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams', 'type_1_teams',\n",
    "#     'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "# ]\n",
    "# predictors = hazard_vars + values_vars + capacity_vars\n",
    "\n",
    "# Descriptive statistics table\n",
    "desc = matched_clean[predictors].describe().T[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "desc.to_csv('descriptive_stats_table.csv')\n",
    "print(desc)\n",
    "\n",
    "# Correlation matrix plot\n",
    "corr = matched_clean[predictors].corr()\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "cax = ax.imshow(corr, interpolation='nearest')\n",
    "ax.set_xticks(range(len(predictors)))\n",
    "ax.set_yticks(range(len(predictors)))\n",
    "ax.set_xticklabels(predictors, rotation=90)\n",
    "ax.set_yticklabels(predictors)\n",
    "fig.colorbar(cax)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d90c9-5ca3-4881-b6c0-4aa09e066ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# corr = matched_clean[predictors].corr()\n",
    "# plt.figure(figsize=(12,10))\n",
    "# sns.heatmap(corr, annot=False, cmap='coolwarm', center=0)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_vif = sm.add_constant(matched_clean[predictors])\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i)\n",
    "                   for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b200a84-1064-46bd-a619-d83b39d1c702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208cf59-5999-469a-be42-f76f1a000c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_clean['sph_Normal'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1ff11-20d7-4f65-aa26-523ea09b6f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe84ee-70eb-499f-9606-58cdbbfae004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + values_vars + capacity_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97888786-5650-4606-81c3-897a2715eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Prepare data\n",
    "# matched_clean = matched_clean.copy()  # ensure fresh copy\n",
    "# X = matched_clean[predictors]\n",
    "# y = matched_clean['event_type']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Scale features using training data only\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# # 1) Statsmodels Logit\n",
    "# X_train_sm = sm.add_constant(X_train)\n",
    "# logit_train = sm.Logit(y_train, X_train_sm).fit(disp=False)\n",
    "# print(logit_train.summary())\n",
    "\n",
    "# X_test_sm = sm.add_constant(X_test)\n",
    "# y_proba = logit_train.predict(X_test_sm)\n",
    "# print(\"AUC (Logit):\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# best_t, best_f = 0, 0\n",
    "# for t in [i/100 for i in range(1,100)]:\n",
    "#     f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "#     if f > best_f:\n",
    "#         best_f, best_t = f, t\n",
    "# print(f\"Best threshold: {best_t}, F1: {best_f}\")\n",
    "# print(classification_report(y_test, (y_proba > best_t).astype(int)))\n",
    "\n",
    "# # 2) Class-weighted Logistic Regression\n",
    "# lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "# lr.fit(X_train, y_train)\n",
    "# y_proba_w = lr.predict_proba(X_test)[:,1]\n",
    "# print(\"AUC (class_weight LR):\", roc_auc_score(y_test, y_proba_w))\n",
    "\n",
    "# best_t_w, best_f_w = 0, 0\n",
    "# for t in [i/100 for i in range(1,100)]:\n",
    "#     f = f1_score(y_test, (y_proba_w > t).astype(int))\n",
    "#     if f > best_f_w:\n",
    "#         best_f_w, best_t_w = f, t\n",
    "# print(f\"Best threshold (weighted): {best_t_w}, F1: {best_f_w}\")\n",
    "# print(classification_report(y_test, (y_proba_w > best_t_w).astype(int)))\n",
    "\n",
    "# # 3) Manual Undersampling\n",
    "# train_df = pd.concat([X_train, y_train], axis=1)\n",
    "# maj = train_df[train_df['event_type']==0]\n",
    "# minn = train_df[train_df['event_type']==1]\n",
    "# maj_down = maj.sample(n=len(minn), random_state=42)\n",
    "# balanced = pd.concat([maj_down, minn])\n",
    "# X_bal, y_bal = balanced[predictors], balanced['event_type']\n",
    "# lr2 = LogisticRegression(max_iter=1000)\n",
    "# lr2.fit(X_bal, y_bal)\n",
    "# y_proba_s = lr2.predict_proba(X_test)[:,1]\n",
    "# print(\"AUC (undersampled LR):\", roc_auc_score(y_test, y_proba_s))\n",
    "\n",
    "# best_t_s, best_f_s = 0, 0\n",
    "# for t in [i/100 for i in range(1,100)]:\n",
    "#     f = f1_score(y_test, (y_proba_s > t).astype(int))\n",
    "#     if f > best_f_s:\n",
    "#         best_f_s, best_t_s = f, t\n",
    "# print(f\"Best threshold (undersampled): {best_t_s}, F1: {best_f_s}\")\n",
    "# print(classification_report(y_test, (y_proba_s > best_t_s).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa27824-a0a5-4714-b6f1-87d8d38dd1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816ffe3-c04a-432d-8c9f-f6b56815d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type']\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "# 1) Statsmodels Logit (with named features)\n",
    "X_train_sm = sm.add_constant(X_train_scaled)\n",
    "logit_train = sm.Logit(y_train, X_train_sm).fit(disp=False)\n",
    "print(logit_train.summary())\n",
    "\n",
    "X_test_sm = sm.add_constant(X_test_scaled)\n",
    "y_proba = logit_train.predict(X_test_sm)\n",
    "print(\"AUC (Logit):\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "best_t, best_f = 0, 0\n",
    "for t in [i / 100 for i in range(1, 100)]:\n",
    "    f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "    if f > best_f:\n",
    "        best_f, best_t = f, t\n",
    "print(f\"Best threshold: {best_t}, F1: {best_f}\")\n",
    "print(classification_report(y_test, (y_proba > best_t).astype(int)))\n",
    "\n",
    "# 2) Class-weighted Logistic Regression\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_proba_w = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "print(\"AUC (class_weight LR):\", roc_auc_score(y_test, y_proba_w))\n",
    "\n",
    "best_t_w, best_f_w = 0, 0\n",
    "for t in [i / 100 for i in range(1, 100)]:\n",
    "    f = f1_score(y_test, (y_proba_w > t).astype(int))\n",
    "    if f > best_f_w:\n",
    "        best_f_w, best_t_w = f, t\n",
    "print(f\"Best threshold (weighted): {best_t_w}, F1: {best_f_w}\")\n",
    "print(classification_report(y_test, (y_proba_w > best_t_w).astype(int)))\n",
    "\n",
    "# 3) Manual Undersampling (using scaled data with column names preserved)\n",
    "train_df = pd.concat([X_train_scaled, y_train], axis=1)\n",
    "maj = train_df[train_df['event_type'] == 0]\n",
    "minn = train_df[train_df['event_type'] == 1]\n",
    "maj_down = maj.sample(n=len(minn), random_state=42)\n",
    "balanced = pd.concat([maj_down, minn])\n",
    "\n",
    "X_bal = balanced.drop(columns='event_type')\n",
    "y_bal = balanced['event_type']\n",
    "\n",
    "lr2 = LogisticRegression(max_iter=1000)\n",
    "lr2.fit(X_bal, y_bal)\n",
    "y_proba_s = lr2.predict_proba(X_test_scaled)[:, 1]\n",
    "print(\"AUC (undersampled LR):\", roc_auc_score(y_test, y_proba_s))\n",
    "\n",
    "best_t_s, best_f_s = 0, 0\n",
    "for t in [i / 100 for i in range(1, 100)]:\n",
    "    f = f1_score(y_test, (y_proba_s > t).astype(int))\n",
    "    if f > best_f_s:\n",
    "        best_f_s, best_t_s = f, t\n",
    "print(f\"Best threshold (undersampled): {best_t_s}, F1: {best_f_s}\")\n",
    "print(classification_report(y_test, (y_proba_s > best_t_s).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1c168-0f6d-4255-b784-9d4ed22596bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type']\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "# Class-weighted Logistic Regression\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# AUC\n",
    "print(\"AUC (class_weight LR):\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# Find best threshold by F1\n",
    "best_t, best_f = 0, 0\n",
    "for t in [i / 100 for i in range(1, 100)]:\n",
    "    f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "    if f > best_f:\n",
    "        best_f, best_t = f, t\n",
    "print(f\"Best threshold (weighted): {best_t}, F1: {best_f}\")\n",
    "\n",
    "# Classification report at best threshold\n",
    "print(classification_report(y_test, (y_proba > best_t).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f3a35-45f5-45d8-8bde-af4005f347eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d65f26-3337-43b7-bd5c-2f48608fa771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ac62a-8db7-4f48-bfc0-6398e09f475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d6644-8cec-4245-b968-8b8ac5b7478a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34cd35-7fb1-41da-9e58-a3482d22d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42339645-249b-47a5-9c1a-8028691c048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + values_vars + capacity_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44668dc-9b0d-457a-ba09-364db22b4679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1121, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1237, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1121, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1237, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1121, in __getitem__\n",
      "    return self._get_value(key)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1237, in _get_value\n",
      "    loc = self.index.get_loc(label)\n",
      "  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1121, in __getitem__\n    return self._get_value(key)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1237, in _get_value\n    loc = self.index.get_loc(label)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 90\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(tuner_dir): shutil\u001b[38;5;241m.\u001b[39mrmtree(tuner_dir)\n\u001b[0;32m     86\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m     87\u001b[0m     build_model, objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     88\u001b[0m     directory\u001b[38;5;241m=\u001b[39mtuner_dir, project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn_run\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m )\n\u001b[1;32m---> 90\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     97\u001b[0m y_proba_nn \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:235\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:339\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[1;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[0;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:588\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\oracle.py:545\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    543\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    549\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1121, in __getitem__\n    return self._get_value(key)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\series.py\", line 1237, in _get_value\n    loc = self.index.get_loc(label)\n  File \"C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 0\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import roc_auc_score, f1_score\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import os, shutil\n",
    "\n",
    "# # assume matched_clean (DataFrame) and predictors (list of column names) are defined\n",
    "# X = matched_clean[predictors]\n",
    "# y = matched_clean['event_type']\n",
    "\n",
    "# class F1Metric(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "#         super().__init__(name=name, **kwargs)\n",
    "#         self.threshold = threshold\n",
    "#         self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "#         self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "#         self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "#         self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "#         self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "#     def result(self):\n",
    "#         precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "#         recall    = self.tp / (self.tp + self.fn + 1e-8)\n",
    "#         return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "#     def reset_states(self):\n",
    "#         for v in self.variables:\n",
    "#             v.assign(0)\n",
    "\n",
    "# def build_model(hp):\n",
    "#     hp_units = hp.Int('units', min_value=16, max_value=256, step=16)\n",
    "#     hp_lr    = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     model = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(hp_units, activation='relu'),\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "# lr_auc_scores, lr_f1_scores = [], []\n",
    "# nn_auc_scores, nn_f1_scores = [], []\n",
    "\n",
    "# for i, (train_idx, test_idx) in enumerate(sss.split(X, y)):\n",
    "#     train_df = matched_clean.iloc[train_idx]\n",
    "#     test_df  = matched_clean.iloc[test_idx]\n",
    "#     X_train, y_train = train_df[predictors], train_df['event_type']\n",
    "#     X_test,  y_test  = test_df[predictors],  test_df['event_type']\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "#     # Logistic Regression\n",
    "#     lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "#     lr.fit(X_train_scaled, y_train)\n",
    "#     y_proba_lr = lr.predict_proba(X_test_scaled)[:,1]\n",
    "#     auc_lr = roc_auc_score(y_test, y_proba_lr)\n",
    "#     best_f1_lr, best_t_lr = 0, 0\n",
    "#     for t in np.linspace(0.01, 0.99, 99):\n",
    "#         f = f1_score(y_test, (y_proba_lr > t).astype(int))\n",
    "#         if f > best_f1_lr:\n",
    "#             best_f1_lr, best_t_lr = f, t\n",
    "#     lr_auc_scores.append(auc_lr)\n",
    "#     lr_f1_scores.append(best_f1_lr)\n",
    "\n",
    "#     # Neural Network\n",
    "#     classes = np.unique(y_train)\n",
    "#     weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "#     class_weights = dict(zip(classes, weights))\n",
    "#     tuner_dir = f\"keras_tuner_dir/run_{i}\"\n",
    "#     if os.path.exists(tuner_dir): shutil.rmtree(tuner_dir)\n",
    "#     tuner = kt.RandomSearch(\n",
    "#         build_model, objective='val_auc', max_trials=10,\n",
    "#         directory=tuner_dir, project_name='nn_run'\n",
    "#     )\n",
    "#     tuner.search(\n",
    "#         X_train_scaled, y_train,\n",
    "#         epochs=5, batch_size=512, validation_split=0.2,\n",
    "#         callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "#         class_weight=class_weights, verbose=0\n",
    "#     )\n",
    "#     best_model = tuner.get_best_models(1)[0]\n",
    "#     y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "#     auc_nn = roc_auc_score(y_test, y_proba_nn)\n",
    "#     best_f1_nn, best_t_nn = 0, 0\n",
    "#     for t in np.linspace(0.01, 0.99, 99):\n",
    "#         f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "#         if f > best_f1_nn:\n",
    "#             best_f1_nn, best_t_nn = f, t\n",
    "#     nn_auc_scores.append(auc_nn)\n",
    "#     nn_f1_scores.append(best_f1_nn)\n",
    "\n",
    "# print(f\"LR AUC: {np.mean(lr_auc_scores):.3f} ± {np.std(lr_auc_scores):.3f}\")\n",
    "# print(f\"LR F1 : {np.mean(lr_f1_scores):.3f} ± {np.std(lr_f1_scores):.3f}\")\n",
    "# print(f\"NN AUC: {np.mean(nn_auc_scores):.3f} ± {np.std(nn_auc_scores):.3f}\")\n",
    "# print(f\"NN F1 : {np.mean(nn_f1_scores):.3f} ± {np.std(nn_f1_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afcb34e5-62ea-41da-a3e1-21febb74537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (class_weight LR): 0.79576778949017\n",
      "Best threshold (weighted): 0.75, F1: 0.14549116826774094\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96    130631\n",
      "           1       0.09      0.32      0.15      2902\n",
      "\n",
      "    accuracy                           0.92    133533\n",
      "   macro avg       0.54      0.63      0.55    133533\n",
      "weighted avg       0.96      0.92      0.94    133533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type']\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "# Class-weighted Logistic Regression\n",
    "lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# AUC\n",
    "print(\"AUC (class_weight LR):\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# Find best threshold by F1\n",
    "best_t, best_f = 0, 0\n",
    "for t in [i / 100 for i in range(1, 100)]:\n",
    "    f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "    if f > best_f:\n",
    "        best_f, best_t = f, t\n",
    "print(f\"Best threshold (weighted): {best_t}, F1: {best_f}\")\n",
    "\n",
    "# Classification report at best threshold\n",
    "print(classification_report(y_test, (y_proba > best_t).astype(int)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "\n",
    "# # Initialize storage\n",
    "# auc_scores = []\n",
    "# f1_scores = []\n",
    "# thresholds = []\n",
    "# reports = []\n",
    "\n",
    "# # Stratified splitting setup\n",
    "# sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "\n",
    "# for i, (train_idx, test_idx) in enumerate(sss.split(matched_clean, matched_clean['event_type'])):\n",
    "#     train_df = matched_clean.iloc[train_idx]\n",
    "#     test_df = matched_clean.iloc[test_idx]\n",
    "\n",
    "#     X_train = train_df[predictors]\n",
    "#     y_train = train_df['event_type'].reset_index(drop=True)\n",
    "\n",
    "#     X_test = test_df[predictors]\n",
    "#     y_test = test_df['event_type'].reset_index(drop=True)\n",
    "\n",
    "#     # Scale features\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "#     X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "#     # Train class-weighted logistic regression\n",
    "#     lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "#     lr.fit(X_train_scaled, y_train)\n",
    "#     y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "#     # Compute AUC\n",
    "#     auc = roc_auc_score(y_test, y_proba)\n",
    "#     auc_scores.append(auc)\n",
    "\n",
    "#     # Find best threshold for F1\n",
    "#     best_f1, best_t = 0, 0\n",
    "#     for t in np.linspace(0.01, 0.99, 99):\n",
    "#         preds = (y_proba > t).astype(int)\n",
    "#         f = f1_score(y_test, preds)\n",
    "#         if f > best_f1:\n",
    "#             best_f1 = f\n",
    "#             best_t = t\n",
    "\n",
    "#     f1_scores.append(best_f1)\n",
    "#     thresholds.append(best_t)\n",
    "\n",
    "#     # Save classification report\n",
    "#     report = classification_report(y_test, (y_proba > best_t).astype(int), output_dict=True)\n",
    "#     reports.append(report)\n",
    "\n",
    "# # Summary\n",
    "# print(f\"\\nLogistic Regression over 10 runs:\")\n",
    "# print(f\"AUC: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
    "# print(f\"F1:  {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "# print(f\"Best thresholds (mean): {np.mean(thresholds):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900161c6-5ae6-45ef-ac51-f240b3acf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic regression\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Prepare data\n",
    "# matched_clean = matched_clean.copy()\n",
    "# X = matched_clean[predictors]\n",
    "# y = matched_clean['event_type']\n",
    "# X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # Scale features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "# # Class-weighted Logistic Regression\n",
    "# lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "# lr.fit(X_train_scaled, y_train)\n",
    "# y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# # AUC\n",
    "# print(\"AUC (class_weight LR):\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# # Find best threshold by F1\n",
    "# best_t, best_f = 0, 0\n",
    "# for t in [i / 100 for i in range(1, 100)]:\n",
    "#     f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "#     if f > best_f:\n",
    "#         best_f, best_t = f, t\n",
    "# print(f\"Best threshold (weighted): {best_t}, F1: {best_f}\")\n",
    "\n",
    "# # Classification report at best threshold\n",
    "# print(classification_report(y_test, (y_proba > best_t).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe741149-cdd1-4469-9b29-b5001585ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 23s]\n",
      "val_auc: 0.8161504864692688\n",
      "\n",
      "Best val_auc So Far: 0.825941264629364\n",
      "Total elapsed time: 00h 08m 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,888</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m)                 │           \u001b[38;5;34m3,888\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m145\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,033</span> (15.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,033\u001b[0m (15.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,033</span> (15.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,033\u001b[0m (15.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "units: 144\n",
      "lr: 0.005418161911653196\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step\n",
      "AUC (NN): 0.824657841799013\n",
      "Best threshold (NN): 0.8, F1: 0.17576923076923076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97    130631\n",
      "           1       0.12      0.31      0.18      2902\n",
      "\n",
      "    accuracy                           0.94    133533\n",
      "   macro avg       0.55      0.63      0.57    133533\n",
      "weighted avg       0.97      0.94      0.95    133533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "# --- Custom F1 metric ---------------------------------------------------------\n",
    "class F1Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "        recall   = self.tp / (self.tp + self.fn + 1e-8)\n",
    "        return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(0)\n",
    "\n",
    "# --- Model builder -----------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    hp_units = hp.Int('units', min_value=16, max_value=256, step=16)\n",
    "    hp_lr    = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(hp_units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Hyperparameter tuning ----------------------------------------------------\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=20,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='one_hidden_layer_5'\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluation ---------------------------------------------------------------\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Print model architecture\n",
    "best_model.summary()\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print all tuned hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for param in best_hp.values:\n",
    "    print(f\"{param}: {best_hp.get(param)}\")\n",
    "\n",
    "    \n",
    "\n",
    "y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "print(\"AUC (NN):\", roc_auc_score(y_test, y_proba_nn))\n",
    "\n",
    "best_t_nn, best_f_nn = 0, 0\n",
    "for t in np.linspace(0.01, 0.99, 99):\n",
    "    f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "    if f > best_f_nn:\n",
    "        best_f_nn, best_t_nn = f, t\n",
    "\n",
    "print(f\"Best threshold (NN): {best_t_nn}, F1: {best_f_nn}\")\n",
    "print(classification_report(y_test, (y_proba_nn > best_t_nn).astype(int)))\n",
    "\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # --- Custom F1 metric ---------------------------------------------------------\n",
    "# class F1Metric(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "#         super().__init__(name=name, **kwargs)\n",
    "#         self.threshold = threshold\n",
    "#         self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "#         self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "#         self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "#         self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "#         self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "#     def result(self):\n",
    "#         precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "#         recall   = self.tp / (self.tp + self.fn + 1e-8)\n",
    "#         return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "#     def reset_states(self):\n",
    "#         for v in self.variables:\n",
    "#             v.assign(0)\n",
    "\n",
    "# # --- Model builder -----------------------------------------------------------\n",
    "# def build_model(hp):\n",
    "#     hp_units = hp.Int('units', min_value=16, max_value=256, step=16)\n",
    "#     hp_lr    = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     model = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(hp_units, activation='relu'),\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # --- Repeated tuning runs ----------------------------------------------------\n",
    "# n_runs = 10\n",
    "# auc_list = []\n",
    "# f1_list = []\n",
    "\n",
    "# for i in range(n_runs):\n",
    "#     print(f\"\\n--- Run {i+1}/{n_runs} ---\")\n",
    "\n",
    "#     # Split and scale\n",
    "#     X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "#         X, y, stratify=y, test_size=0.3, random_state=i)\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "#     X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "#     # Class weights\n",
    "#     classes = np.unique(y_train)\n",
    "#     weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "#     class_weights = dict(zip(classes, weights))\n",
    "\n",
    "#     # Clear old tuner directory\n",
    "#     tuner_dir = f\"keras_tuner_dir/run_{i}\"\n",
    "#     if os.path.exists(tuner_dir):\n",
    "#         shutil.rmtree(tuner_dir)\n",
    "\n",
    "#     tuner = kt.RandomSearch(\n",
    "#         build_model,\n",
    "#         objective='val_auc',\n",
    "#         max_trials=2,\n",
    "#         directory=tuner_dir,\n",
    "#         project_name='one_hidden_layer4'\n",
    "#     )\n",
    "\n",
    "#     early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "#     tuner.search(\n",
    "#         X_train_scaled, y_train,\n",
    "#         epochs=3,\n",
    "#         batch_size=512,\n",
    "#         validation_split=0.2,\n",
    "#         callbacks=[early_stop],\n",
    "#         class_weight=class_weights,\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     best_model = tuner.get_best_models(1)[0]\n",
    "#     y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "#     auc = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "#     best_t, best_f = 0, 0\n",
    "#     for t in np.linspace(0.01, 0.99, 99):\n",
    "#         f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "#         if f > best_f:\n",
    "#             best_f, best_t = f, t\n",
    "\n",
    "#     print(f\"AUC: {auc:.3f}, Best F1: {best_f:.3f} @ threshold {best_t:.2f}\")\n",
    "#     auc_list.append(auc)\n",
    "#     f1_list.append(best_f)\n",
    "\n",
    "# # --- Summary ----------------------------------------------------\n",
    "# print(\"\\n--- Final Summary (10 runs) ---\")\n",
    "# print(f\"Mean AUC: {np.mean(auc_list):.3f} ± {np.std(auc_list):.3f}\")\n",
    "# print(f\"Mean F1 : {np.mean(f1_list):.3f} ± {np.std(f1_list):.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "# import os, shutil, numpy as np, tensorflow as tf, keras_tuner as kt\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.metrics import roc_auc_score, f1_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# #  DATA  (assume matched_clean DataFrame and predictors list exist)\n",
    "# # ------------------------------------------------------------------\n",
    "# X = matched_clean[predictors]\n",
    "# y = matched_clean[\"event_type\"]\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# #  CUSTOM F1 METRIC FOR KERAS\n",
    "# # ------------------------------------------------------------------\n",
    "# class F1Metric(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, name=\"f1\", threshold=0.5, **kwargs):\n",
    "#         super().__init__(name=name, **kwargs)\n",
    "#         self.threshold = threshold\n",
    "#         self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "#         self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "#         self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "#         self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "#         self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "#     def result(self):\n",
    "#         precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "#         recall    = self.tp / (self.tp + self.fn + 1e-8)\n",
    "#         return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "#     def reset_states(self):\n",
    "#         for v in self.variables:\n",
    "#             v.assign(0)\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# #  KERAS‑TUNER MODEL BUILDER\n",
    "# # ------------------------------------------------------------------\n",
    "# def build_model(hp):\n",
    "#     hp_units = hp.Int(\"units\", 16, 256, step=16)\n",
    "#     hp_lr    = hp.Float(\"lr\", 1e-4, 1e-2, sampling=\"log\")\n",
    "#     model = models.Sequential(\n",
    "#         [\n",
    "#             layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#             layers.Dense(hp_units, activation=\"relu\"),\n",
    "#             layers.Dense(1, activation=\"sigmoid\"),\n",
    "#         ]\n",
    "#     )\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[tf.keras.metrics.AUC(name=\"auc\"), F1Metric()],\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# #  CROSS‑VALIDATION LOOP (LR + NN on identical folds)\n",
    "# # ------------------------------------------------------------------\n",
    "# sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n",
    "# lr_auc, lr_f1, nn_auc, nn_f1 = [], [], [], []\n",
    "\n",
    "# for fold, (train_idx, test_idx) in enumerate(sss.split(X, y)):\n",
    "#     train_df, test_df = matched_clean.iloc[train_idx], matched_clean.iloc[test_idx]\n",
    "#     X_train, y_train  = train_df[predictors], train_df[\"event_type\"]\n",
    "#     X_test,  y_test   = test_df[predictors],  test_df[\"event_type\"]\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "#     # ---------------- LOGISTIC REGRESSION ----------------\n",
    "#     lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "#     lr.fit(X_train_scaled, y_train)\n",
    "#     lr_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "#     lr_auc.append(roc_auc_score(y_test, lr_proba))\n",
    "#     lr_f1.append(max(f1_score(y_test, (lr_proba > t).astype(int))\n",
    "#                      for t in np.linspace(0.01, 0.99, 99)))\n",
    "\n",
    "#     # ---------------- NEURAL NET (KERAS‑TUNER) ----------------\n",
    "#     classes  = np.unique(y_train)\n",
    "#     cw       = dict(zip(classes, compute_class_weight(\"balanced\", classes, y_train)))\n",
    "#     tuner_dir = f\"kt_run_{fold}\"\n",
    "#     if os.path.exists(tuner_dir): shutil.rmtree(tuner_dir)\n",
    "\n",
    "#     tuner = kt.RandomSearch(\n",
    "#         build_model,\n",
    "#         objective=\"val_auc\",\n",
    "#         max_trials=5,\n",
    "#         overwrite=True,\n",
    "#         directory=tuner_dir,\n",
    "#         project_name=\"nn\",\n",
    "#     )\n",
    "\n",
    "#     es = callbacks.EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "#     tuner.search(\n",
    "#         X_train_scaled,\n",
    "#         y_train,\n",
    "#         epochs=5,\n",
    "#         batch_size=512,\n",
    "#         validation_split=0.2,\n",
    "#         class_weight=cw,\n",
    "#         callbacks=[es],\n",
    "#         verbose=0,\n",
    "#     )\n",
    "\n",
    "#     best_hp   = tuner.get_best_hyperparameters(1)[0]\n",
    "#     best_model = build_model(best_hp)\n",
    "#     best_model.fit(\n",
    "#         X_train_scaled,\n",
    "#         y_train,\n",
    "#         epochs=5,\n",
    "#         batch_size=512,\n",
    "#         validation_split=0.2,\n",
    "#         class_weight=cw,\n",
    "#         callbacks=[es],\n",
    "#         verbose=0,\n",
    "#     )\n",
    "\n",
    "#     nn_proba = best_model.predict(X_test_scaled).flatten()\n",
    "#     nn_auc.append(roc_auc_score(y_test, nn_proba))\n",
    "#     nn_f1.append(max(f1_score(y_test, (nn_proba > t).astype(int))\n",
    "#                      for t in np.linspace(0.01, 0.99, 99)))\n",
    "\n",
    "# # ------------------------------------------------------------------\n",
    "# #  RESULTS\n",
    "# # ------------------------------------------------------------------\n",
    "# print(f\"LR  AUC: {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "# print(f\"LR  F1 : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "# print(f\"NN  AUC: {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "# print(f\"NN  F1 : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bae18dc2-1e58-47a6-a203-6aaf31fd91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 11s]\n",
      "val_auc: 0.8315985202789307\n",
      "\n",
      "Best val_auc So Far: 0.8315985202789307\n",
      "Total elapsed time: 00h 11m 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,912</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">57,568</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m6,912\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)                 │          \u001b[38;5;34m57,568\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m225\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,705</span> (252.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,705\u001b[0m (252.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,705</span> (252.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,705\u001b[0m (252.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "units_1: 256\n",
      "units_2: 224\n",
      "lr: 0.001141937009820313\n",
      "\u001b[1m4173/4173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step\n",
      "AUC (NN): 0.8338030391750468\n",
      "Best threshold (NN): 0.8200000000000001, F1: 0.19841066393232504\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98    130631\n",
      "           1       0.16      0.27      0.20      2902\n",
      "\n",
      "    accuracy                           0.95    133533\n",
      "   macro avg       0.57      0.62      0.59    133533\n",
      "weighted avg       0.97      0.95      0.96    133533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "\n",
    "# --- Custom F1 metric ---------------------------------------------------------\n",
    "class F1Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "        recall   = self.tp / (self.tp + self.fn + 1e-8)\n",
    "        return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(0)\n",
    "\n",
    "# --- Model builder -----------------------------------------------------------\n",
    "# --- Model builder -----------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    hp_units_1 = hp.Int('units_1', min_value=16, max_value=256, step=16)\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=256, step=16)\n",
    "    hp_lr      = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(hp_units_1, activation='relu'),\n",
    "        layers.Dense(hp_units_2, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Hyperparameter tuning ----------------------------------------------------\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=20,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='two_hidden_layers_test2'\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluation ---------------------------------------------------------------\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Print model architecture\n",
    "best_model.summary()\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print all tuned hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for param in best_hp.values:\n",
    "    print(f\"{param}: {best_hp.get(param)}\")\n",
    "\n",
    "    \n",
    "\n",
    "y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "print(\"AUC (NN):\", roc_auc_score(y_test, y_proba_nn))\n",
    "\n",
    "best_t_nn, best_f_nn = 0, 0\n",
    "for t in np.linspace(0.01, 0.99, 99):\n",
    "    f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "    if f > best_f_nn:\n",
    "        best_f_nn, best_t_nn = f, t\n",
    "\n",
    "print(f\"Best threshold (NN): {best_t_nn}, F1: {best_f_nn}\")\n",
    "print(classification_report(y_test, (y_proba_nn > best_t_nn).astype(int)))\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras_tuner as kt\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # --- Custom F1 metric ---------------------------------------------------------\n",
    "# class F1Metric(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "#         super().__init__(name=name, **kwargs)\n",
    "#         self.threshold = threshold\n",
    "#         self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "#         self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "#         self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "#         self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "#         self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "#     def result(self):\n",
    "#         precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "#         recall   = self.tp / (self.tp + self.fn + 1e-8)\n",
    "#         return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "#     def reset_states(self):\n",
    "#         for v in self.variables:\n",
    "#             v.assign(0)\n",
    "\n",
    "# # --- Model builder -----------------------------------------------------------\n",
    "# def build_model(hp):\n",
    "#     hp_units_1 = hp.Int('units_1', min_value=16, max_value=256, step=16)\n",
    "#     hp_units_2 = hp.Int('units_2', min_value=16, max_value=256, step=16)\n",
    "#     hp_lr = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     model = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(hp_units_1, activation='relu'),\n",
    "#         layers.Dense(hp_units_2, activation='relu'),\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # --- Repeated tuning runs ----------------------------------------------------\n",
    "# n_runs = 10\n",
    "# auc_list = []\n",
    "# f1_list = []\n",
    "\n",
    "# for i in range(n_runs):\n",
    "#     print(f\"\\n--- Run {i+1}/{n_runs} ---\")\n",
    "\n",
    "#     # Split and scale\n",
    "#     X_train_df, X_test_df, y_train_split, y_test = train_test_split(\n",
    "#         X, y, stratify=y, test_size=0.3, random_state=i)\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "#     X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "#     # Class weights\n",
    "#     classes = np.unique(y_train_split)\n",
    "#     weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_split)\n",
    "#     class_weights = dict(zip(classes, weights))\n",
    "\n",
    "#     # Clear tuner directory\n",
    "#     tuner_dir = f\"keras_tuner_dir/two_hidden_run_{i}\"\n",
    "#     if os.path.exists(tuner_dir):\n",
    "#         shutil.rmtree(tuner_dir)\n",
    "\n",
    "#     tuner = kt.RandomSearch(\n",
    "#         build_model,\n",
    "#         objective='val_auc',\n",
    "#         max_trials=20,\n",
    "#         directory=tuner_dir,\n",
    "#         project_name='two_hidden_layers3'\n",
    "#     )\n",
    "\n",
    "#     tuner.search(\n",
    "#         X_train_scaled, y_train_split,\n",
    "#         epochs=3,\n",
    "#         batch_size=512,\n",
    "#         validation_split=0.2,\n",
    "#         callbacks=[callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)],\n",
    "#         class_weight=class_weights,\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     best_model = tuner.get_best_models(1)[0]\n",
    "#     y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "#     auc = roc_auc_score(y_test, y_proba_nn)\n",
    "\n",
    "#     best_t, best_f = 0, 0\n",
    "#     for t in np.linspace(0.01, 0.99, 99):\n",
    "#         f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "#         if f > best_f:\n",
    "#             best_f, best_t = f, t\n",
    "\n",
    "#     print(f\"AUC: {auc:.3f}, Best F1: {best_f:.3f} @ threshold {best_t:.2f}\")\n",
    "#     auc_list.append(auc)\n",
    "#     f1_list.append(best_f)\n",
    "\n",
    "# # --- Summary ----------------------------------------------------\n",
    "# print(\"\\n--- Final Summary (10 runs, 2 hidden layers) ---\")\n",
    "# print(f\"Mean AUC: {np.mean(auc_list):.3f} ± {np.std(auc_list):.3f}\")\n",
    "# print(f\"Mean F1 : {np.mean(f1_list):.3f} ± {np.std(f1_list):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b010a-8e75-4472-b457-d18158f3769a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49624d-5e7c-417a-b05a-ad9369527486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfe53f-fa6b-432a-9077-c34b9d837ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5de50e-4bd1-4b14-8dc7-734370ab6600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
