{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743ea52-2645-44ac-8797-97671db634b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba82916-c99d-4d1a-8f4f-2200fd6902d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\AppData\\Local\\Temp\\ipykernel_25328\\3623525150.py:11: DtypeWarning: Columns (14,15,16,17,311) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"D:\\Aviation\\IMSR\\final_df_resource_summary_AND_gacc_activity_AND_national_activity_2015_2021_WITH_water_lines_cooridors.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455960\n",
      "Cleaned matched_clean length: 445107\n",
      "Event type counts:\n",
      "event_type\n",
      "0    435433\n",
      "1      9674\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of unique event types: 2\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV\n",
    "# df = pd.read_csv(r\"D:\\Aviation\\IMSR\\final_df_resource_summary_AND_gacc_activity_AND_national_activity_2015_2021_WITH_water_lines_cooridors.csv\")\n",
    "\n",
    "# # Print column names\n",
    "# print(df.columns.tolist())\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv(r\"D:\\Aviation\\IMSR\\final_df_resource_summary_AND_gacc_activity_AND_national_activity_2015_2021_WITH_water_lines_cooridors.csv\")\n",
    "\n",
    "# Print column names\n",
    "#print(df.columns.tolist())\n",
    "\n",
    "df = df[\n",
    "    (df['RPL_THEMES'] != -999) &\n",
    "    (df['EPL_MUNIT'] != -999)\n",
    "]\n",
    "\n",
    "mapping = {'LIGHT': 0, 'MODERATE': 1, 'HEAVY': 2}\n",
    "df['initial_attack_activity'] = df['initial_attack_activity'].map(mapping)\n",
    "\n",
    "\n",
    "\n",
    "matched = df\n",
    "\n",
    "import numpy as np\n",
    "# Define variable sets\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal', 'fm100_Normal',\n",
    "    'srad_Normal', 'sph_Normal', 'Slope', 'TRI', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + values_vars + capacity_vars\n",
    "\n",
    "# # Define placeholder values and problematic columns\n",
    "# placeholder_vals = [-999, -999.0, -3.402823e+38]\n",
    "# problem_columns = ['SDI', 'GHM', 'RPL_THEMES', 'EPL_MUNIT']\n",
    "\n",
    "# # Print length before cleaning\n",
    "# print(f\"Original matched length: {len(matched)}\")\n",
    "\n",
    "# # Replace known placeholder values with NaN\n",
    "# for col in problem_columns:\n",
    "#     matched[col] = matched[col].replace(placeholder_vals, np.nan)\n",
    "\n",
    "print(len(matched))\n",
    "\n",
    "# Drop rows where SDI or GHM are < 0\n",
    "#matched = matched[(matched['SDI'] > 0) & (matched['GHM'] > 0)]\n",
    "\n",
    "matched['SDI'] = matched['SDI'].where(matched['SDI'] >= 0, 0)\n",
    "matched['GHM'] = matched['GHM'].where(matched['GHM'] >= 0, 0)\n",
    "\n",
    "# Drop rows with missing predictor or target values\n",
    "matched_clean = matched.dropna(subset=predictors + ['event_type'])\n",
    "\n",
    "# Print length after cleaning\n",
    "print(f\"Cleaned matched_clean length: {len(matched_clean)}\")\n",
    "\n",
    "# Print unique event_type values and their counts\n",
    "event_counts = matched_clean['event_type'].value_counts()\n",
    "print(\"Event type counts:\")\n",
    "print(event_counts)\n",
    "\n",
    "# Print number of unique event types\n",
    "print(f\"\\nNumber of unique event types: {matched_clean['event_type'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cd367b-66ae-43c2-934b-3d366a782397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median FIRE_SIZE (50/50 split): 0.5\n",
      "Number of fires > 300 acres: 6725\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the median\n",
    "median_fire_size = df['FIRE_SIZE'].median()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['FIRE_SIZE_CLASS'], bins=1000, edgecolor='black', log=True)\n",
    "plt.axvline(x=median_fire_size, color='red', linestyle='--', linewidth=2, label=f'Median = {median_fire_size:.2f}')\n",
    "plt.xlabel('Fire Size (Acres)')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.title('Distribution of FIRE_SIZE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Median FIRE_SIZE (50/50 split):\", df['FIRE_SIZE'].median())\n",
    "count_over_300 = (df['FIRE_SIZE'] > 300).sum()\n",
    "print(\"Number of fires > 300 acres:\", count_over_300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a73b55-cf3f-44a6-b96a-35944dec9e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fires > 300 acres: 12547\n"
     ]
    }
   ],
   "source": [
    "count_over_300 = (df['FIRE_SIZE'] > 100).sum()\n",
    "print(\"Number of fires > 300 acres:\", count_over_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62df0c63-e3e7-4bfc-bdd2-4797cf7c28e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FIRE_SIZE entries (including NaNs): 455960\n",
      "Number of FIRE_SIZE values: 455960\n"
     ]
    }
   ],
   "source": [
    "print(\"Total FIRE_SIZE entries (including NaNs):\", len(df['FIRE_SIZE']))\n",
    "print(\"Number of FIRE_SIZE values:\", df['FIRE_SIZE'].count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc27ea-9e36-4f0e-a7bd-f7d4aef8fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL RUNS RAN 7_15_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178eae97-3f26-48b3-8de1-b072f499d0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22524951-6b05-457e-a865-ab348ff0b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\AppData\\Local\\Temp\\ipykernel_25328\\1494853807.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + values_vars + capacity_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4854a78f-7e7f-4355-8afd-97687ad800af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n",
      "WARNING:tensorflow:From C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.797 | F1: 0.148\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.808 | F1: 0.162\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.792 | F1: 0.148\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.791 | F1: 0.143\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.797 | F1: 0.148\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.795 | F1: 0.143\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.789 | F1: 0.144\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.795 | F1: 0.145\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.798 | F1: 0.155\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.804 | F1: 0.155\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.796 ± 0.006\n",
      "  F1         : 0.149 ± 0.006\n",
      "  Precision  : 0.103 ± 0.016\n",
      "  Recall     : 0.304 ± 0.079\n",
      "  Threshold  : 0.77\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.831 ± 0.008\n",
      "  F1         : 0.182 ± 0.009\n",
      "  Precision  : 0.132 ± 0.010\n",
      "  Recall     : 0.300 ± 0.042\n",
      "  Threshold  : 0.83\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.835 ± 0.005\n",
      "  F1         : 0.190 ± 0.008\n",
      "  Precision  : 0.139 ± 0.010\n",
      "  Recall     : 0.309 ± 0.048\n",
      "  Threshold  : 0.82\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_FULL.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_FULL.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a076b175-1f0b-42e7-a60f-f88fad6f0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + values_vars #+ capacity_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1e4fee-ccf9-4b59-af67-e22673db5c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.794 | F1: 0.142\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.804 | F1: 0.160\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.789 | F1: 0.145\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.789 | F1: 0.140\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.794 | F1: 0.145\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.791 | F1: 0.140\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.788 | F1: 0.140\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.793 | F1: 0.144\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.794 | F1: 0.153\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.803 | F1: 0.150\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.794 ± 0.005\n",
      "  F1         : 0.146 ± 0.006\n",
      "  Precision  : 0.101 ± 0.014\n",
      "  Recall     : 0.291 ± 0.066\n",
      "  Threshold  : 0.77\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.831 ± 0.005\n",
      "  F1         : 0.185 ± 0.006\n",
      "  Precision  : 0.135 ± 0.010\n",
      "  Recall     : 0.302 ± 0.043\n",
      "  Threshold  : 0.83\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.839 ± 0.006\n",
      "  F1         : 0.196 ± 0.010\n",
      "  Precision  : 0.144 ± 0.009\n",
      "  Recall     : 0.314 ± 0.039\n",
      "  Threshold  : 0.85\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_haz_val.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_haz_val.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ad624d-6b8d-4e62-8b2d-adf373b143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars + capacity_vars #+ values_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "284762bf-d424-4484-a942-9750537b899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.695 | F1: 0.099\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.691 | F1: 0.095\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.700 | F1: 0.099\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.683 | F1: 0.097\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.701 | F1: 0.104\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.694 | F1: 0.097\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.672 | F1: 0.091\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.686 | F1: 0.092\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.690 | F1: 0.090\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.698 | F1: 0.094\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.691 ± 0.008\n",
      "  F1         : 0.096 ± 0.004\n",
      "  Precision  : 0.060 ± 0.005\n",
      "  Recall     : 0.246 ± 0.045\n",
      "  Threshold  : 0.66\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.772 ± 0.006\n",
      "  F1         : 0.142 ± 0.006\n",
      "  Precision  : 0.101 ± 0.010\n",
      "  Recall     : 0.251 ± 0.040\n",
      "  Threshold  : 0.78\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.776 ± 0.007\n",
      "  F1         : 0.141 ± 0.005\n",
      "  Precision  : 0.101 ± 0.012\n",
      "  Recall     : 0.254 ± 0.050\n",
      "  Threshold  : 0.75\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_haz_cap.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_haz_cap.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c25cca-69cb-4793-a9e7-3ed9436838d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = values_vars + capacity_vars # hazard_vars + \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d24cf81-705c-4054-a7d5-0a0266b0126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.762 | F1: 0.118\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.777 | F1: 0.133\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.765 | F1: 0.127\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.757 | F1: 0.122\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.768 | F1: 0.123\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.761 | F1: 0.115\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.766 | F1: 0.122\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.768 | F1: 0.114\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.769 | F1: 0.124\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.772 | F1: 0.134\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.767 ± 0.005\n",
      "  F1         : 0.123 ± 0.006\n",
      "  Precision  : 0.084 ± 0.009\n",
      "  Recall     : 0.247 ± 0.058\n",
      "  Threshold  : 0.75\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.773 ± 0.007\n",
      "  F1         : 0.129 ± 0.007\n",
      "  Precision  : 0.085 ± 0.009\n",
      "  Recall     : 0.288 ± 0.064\n",
      "  Threshold  : 0.75\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.774 ± 0.006\n",
      "  F1         : 0.129 ± 0.005\n",
      "  Precision  : 0.083 ± 0.005\n",
      "  Recall     : 0.296 ± 0.042\n",
      "  Threshold  : 0.74\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_val_cap.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_val_cap.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10d84207-97f3-402a-8c19-1049ebe7f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = values_vars# + capacity_vars # hazard_vars + \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f96273-dd1a-4dae-81c3-54534cdf5cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.757 | F1: 0.112\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.772 | F1: 0.125\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.761 | F1: 0.121\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.756 | F1: 0.111\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.763 | F1: 0.114\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.758 | F1: 0.116\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.764 | F1: 0.115\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.765 | F1: 0.107\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.763 | F1: 0.114\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.768 | F1: 0.122\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.763 ± 0.005\n",
      "  F1         : 0.116 ± 0.005\n",
      "  Precision  : 0.076 ± 0.008\n",
      "  Recall     : 0.263 ± 0.085\n",
      "  Threshold  : 0.74\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.773 ± 0.006\n",
      "  F1         : 0.119 ± 0.005\n",
      "  Precision  : 0.073 ± 0.007\n",
      "  Recall     : 0.336 ± 0.047\n",
      "  Threshold  : 0.74\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.772 ± 0.005\n",
      "  F1         : 0.119 ± 0.004\n",
      "  Precision  : 0.075 ± 0.006\n",
      "  Recall     : 0.319 ± 0.084\n",
      "  Threshold  : 0.75\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_val.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_val.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca0d8fc-ab9b-4a7c-80da-1fd1f2e82640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = capacity_vars # hazard_vars +  values_vars +\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d640181a-0ef3-4bd3-a452-00890d4e3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.593 | F1: 0.066\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.582 | F1: 0.075\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.594 | F1: 0.071\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.565 | F1: 0.069\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.578 | F1: 0.076\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.575 | F1: 0.060\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.579 | F1: 0.070\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.579 | F1: 0.077\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.587 | F1: 0.079\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.585 | F1: 0.072\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.582 ± 0.008\n",
      "  F1         : 0.071 ± 0.005\n",
      "  Precision  : 0.051 ± 0.006\n",
      "  Recall     : 0.122 ± 0.026\n",
      "  Threshold  : 0.62\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.587 ± 0.013\n",
      "  F1         : 0.066 ± 0.005\n",
      "  Precision  : 0.042 ± 0.007\n",
      "  Recall     : 0.173 ± 0.043\n",
      "  Threshold  : 0.56\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.587 ± 0.012\n",
      "  F1         : 0.065 ± 0.006\n",
      "  Precision  : 0.039 ± 0.004\n",
      "  Recall     : 0.196 ± 0.036\n",
      "  Threshold  : 0.54\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_cap.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_cap.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf80df71-47e3-47d2-97ec-2f9006ac4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define variable sets\n",
    "\n",
    "matched_clean['sph_Normal_1000'] = matched_clean['sph_Normal'] * 1000\n",
    "\n",
    "\n",
    "hazard_vars = [\n",
    "    'vpd_Normal', 'bi_Normal',\n",
    "    'srad_Normal', 'sph_Normal_1000', 'TPI', 'Elevation',\n",
    "    'erc', 'vs', 'Aridity_index', 'SDI'\n",
    "]\n",
    "\n",
    "values_vars = [\n",
    "    'Popo_1km', 'GHM', 'RPL_THEMES', 'EPL_MUNIT', 'dist_to_water_m', 'dist_to_transline_m'\n",
    "]\n",
    "\n",
    "capacity_vars = [\n",
    "    'NPL', 'GACC_PL', 'new_large_fires', 'type_2_teams',\n",
    "    'type_1_teams', 'nimos', 'initial_attack_activity', 'engines', 'helicopters', 'crews'\n",
    "]\n",
    "\n",
    "predictors = hazard_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1380c75c-90a0-4351-9f09-9cdc357633b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 | AUC: 0.684 | F1: 0.094\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 | AUC: 0.681 | F1: 0.090\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 | AUC: 0.693 | F1: 0.093\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 | AUC: 0.678 | F1: 0.095\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 | AUC: 0.695 | F1: 0.098\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 | AUC: 0.683 | F1: 0.096\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 | AUC: 0.669 | F1: 0.085\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 | AUC: 0.685 | F1: 0.090\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 | AUC: 0.681 | F1: 0.090\n",
      "fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "C:\\Users\\magst\\anaconda3\\envs\\APPENV2\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 | AUC: 0.693 | F1: 0.094\n",
      "\n",
      "Logistic Regression (10‑fold CV)\n",
      "  AUC        : 0.684 ± 0.007\n",
      "  F1         : 0.093 ± 0.004\n",
      "  Precision  : 0.059 ± 0.005\n",
      "  Recall     : 0.227 ± 0.052\n",
      "  Threshold  : 0.67\n",
      "\n",
      "Neural Network (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.772 ± 0.009\n",
      "  F1         : 0.139 ± 0.007\n",
      "  Precision  : 0.099 ± 0.009\n",
      "  Recall     : 0.247 ± 0.041\n",
      "  Threshold  : 0.76\n",
      "\n",
      "Neural Network 2 layer (10‑fold CV w/ tuning)\n",
      "  AUC        : 0.784 ± 0.009\n",
      "  F1         : 0.147 ± 0.007\n",
      "  Precision  : 0.106 ± 0.012\n",
      "  Recall     : 0.260 ± 0.061\n",
      "  Threshold  : 0.75\n",
      "\n",
      "Saved full results to:\n",
      "  C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_haz.csv\n"
     ]
    }
   ],
   "source": [
    "# NEED TO RUN TONIGHT 7_15_2025\n",
    "\n",
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, os, shutil, tempfile, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# def build_model_2layer(hp):\n",
    "#     units = hp.Int('units', 16, 256, 16)\n",
    "#     lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "#     m = models.Sequential([\n",
    "#         layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "#         layers.Dense(units, activation='relu'),\n",
    "#         layers.Dense(units, activation='relu'),  # <-- second hidden layer\n",
    "#         layers.Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "#     return m\n",
    "\n",
    "def build_model_2layer(hp, input_shape):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int('units', 16, 256, 16)\n",
    "    lr_rate = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=tf.keras.optimizers.Adam(lr_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "nn_auc, nn_f1, nn_prec, nn_rec, nn_thr = [], [], [], [], []\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr = [], [], [], [], []\n",
    "nn_auc2, nn_f12, nn_prec2, nn_rec2, nn_thr2 = [], [], [], [], []\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(\"fold\")\n",
    "    # Split\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "\n",
    "    best_t_lr, best_f_lr = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_lr > t)\n",
    "        if f > best_f_lr:\n",
    "            best_f_lr, best_t_lr = f, t\n",
    "    pred_lr = proba_lr > best_t_lr\n",
    "\n",
    "    lr_auc.append(auc_lr)\n",
    "    lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr))\n",
    "    lr_rec.append(recall_score(y_test_fold, pred_lr))\n",
    "    lr_thr.append(best_t_lr)\n",
    "\n",
    "    # ── Neural Network with inner tuning ──────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    classes = np.unique(y_train_fold)\n",
    "    cw = dict(zip(classes, compute_class_weight(class_weight='balanced', classes=classes, y=y_train_fold)))\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    tuner = kt.RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn = tuner.get_best_models(1)[0]\n",
    "    proba_nn = best_nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn   = roc_auc_score(y_test_fold, proba_nn)\n",
    "\n",
    "    best_t_nn, best_f_nn = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn > t)\n",
    "        if f > best_f_nn:\n",
    "            best_f_nn, best_t_nn = f, t\n",
    "    pred_nn = proba_nn > best_t_nn\n",
    "\n",
    "    nn_auc.append(auc_nn)\n",
    "    nn_f1.append(best_f_nn)\n",
    "    nn_prec.append(precision_score(y_test_fold, pred_nn))\n",
    "    nn_rec.append(recall_score(y_test_fold, pred_nn))\n",
    "    nn_thr.append(best_t_nn)\n",
    "\n",
    "\n",
    "        \n",
    "    # ── Neural Network 2 Layer with inner tuning ──────────────────────────────────────\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # tuner = kt.RandomSearch(\n",
    "    #     build_model_2layer,\n",
    "    #     objective='val_auc',\n",
    "    #     max_trials=5,\n",
    "    #     directory=tmp_dir,\n",
    "    #     project_name=f'nn2layer_fold_{fold}'\n",
    "    # )\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model_2layer(hp, input_shape=(X_train_scaled.shape[1],)),\n",
    "        objective='val_auc',\n",
    "        max_trials=20,\n",
    "        directory=tmp_dir,\n",
    "        project_name=f'nn2layer_fold_{fold}'\n",
    "    )\n",
    "    tuner.search(X_train_scaled, y_train_fold,\n",
    "                 epochs=30,\n",
    "                 batch_size=512,\n",
    "                 validation_split=0.2,\n",
    "                 callbacks=[callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "                 class_weight=cw,\n",
    "                 verbose=0)\n",
    "\n",
    "    best_nn2 = tuner.get_best_models(1)[0]\n",
    "    proba_nn2 = best_nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc_nn2   = roc_auc_score(y_test_fold, proba_nn2)\n",
    "\n",
    "    best_t_nn2, best_f_nn2 = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test_fold, proba_nn2 > t)\n",
    "        if f > best_f_nn2:\n",
    "            best_f_nn2, best_t_nn2 = f, t\n",
    "    pred_nn2 = proba_nn2 > best_t_nn2\n",
    "\n",
    "    nn_auc2.append(auc_nn2)\n",
    "    nn_f12.append(best_f_nn2)\n",
    "    nn_prec2.append(precision_score(y_test_fold, pred_nn2))\n",
    "    nn_rec2.append(recall_score(y_test_fold, pred_nn2))\n",
    "    nn_thr2.append(best_t_nn2)\n",
    "\n",
    "    print(f\"Fold {fold} | AUC: {auc_lr:.3f} | F1: {best_f_lr:.3f}\")\n",
    "    \n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression (10‑fold CV)\")\n",
    "print(f\"  AUC        : {np.mean(lr_auc):.3f} ± {np.std(lr_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(lr_f1):.3f} ± {np.std(lr_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(lr_prec):.3f} ± {np.std(lr_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(lr_rec):.3f} ± {np.std(lr_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(lr_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc):.3f} ± {np.std(nn_auc):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f1):.3f} ± {np.std(nn_f1):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec):.3f} ± {np.std(nn_prec):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec):.3f} ± {np.std(nn_rec):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr):.2f}\")\n",
    "\n",
    "print(\"\\nNeural Network 2 layer (10‑fold CV w/ tuning)\")\n",
    "print(f\"  AUC        : {np.mean(nn_auc2):.3f} ± {np.std(nn_auc2):.3f}\")\n",
    "print(f\"  F1         : {np.mean(nn_f12):.3f} ± {np.std(nn_f12):.3f}\")\n",
    "print(f\"  Precision  : {np.mean(nn_prec2):.3f} ± {np.std(nn_prec2):.3f}\")\n",
    "print(f\"  Recall     : {np.mean(nn_rec2):.3f} ± {np.std(nn_rec2):.3f}\")\n",
    "print(f\"  Threshold  : {np.mean(nn_thr2):.2f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# COLLECT RESULTS → DATAFRAME  &  SAVE TO CSV\n",
    "# ------------------------------------------------------------------\n",
    "import datetime\n",
    "\n",
    "n_folds = len(lr_auc)\n",
    "fold_idx = list(range(1, n_folds + 1))\n",
    "\n",
    "dfOUT = pd.DataFrame({\n",
    "    \"fold\":                fold_idx,\n",
    "    \"lr_auc\":              lr_auc,\n",
    "    \"lr_f1\":               lr_f1,\n",
    "    \"lr_precision\":        lr_prec,\n",
    "    \"lr_recall\":           lr_rec,\n",
    "    \"lr_threshold\":        lr_thr,\n",
    "    \n",
    "    \"nn_auc\":              nn_auc,\n",
    "    \"nn_f1\":               nn_f1,\n",
    "    \"nn_precision\":        nn_prec,\n",
    "    \"nn_recall\":           nn_rec,\n",
    "    \"nn_threshold\":        nn_thr,\n",
    "    \n",
    "    \"nn2_auc\":             nn_auc2,\n",
    "    \"nn2_f1\":              nn_f12,\n",
    "    \"nn2_precision\":       nn_prec2,\n",
    "    \"nn2_recall\":          nn_rec2,\n",
    "    \"nn2_threshold\":       nn_thr2,\n",
    "})\n",
    "\n",
    "# ---- add a summary row -------------------------------------------------------\n",
    "summary = {\n",
    "    \"fold\": \"mean±std\",\n",
    "    \"lr_auc\":        f\"{np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}\",\n",
    "    \"lr_f1\":         f\"{np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}\",\n",
    "    \"lr_precision\":  f\"{np.mean(lr_prec):.3f}±{np.std(lr_prec):.3f}\",\n",
    "    \"lr_recall\":     f\"{np.mean(lr_rec):.3f}±{np.std(lr_rec):.3f}\",\n",
    "    \"lr_threshold\":  f\"{np.mean(lr_thr):.2f}\",\n",
    "\n",
    "    \"nn_auc\":        f\"{np.mean(nn_auc):.3f}±{np.std(nn_auc):.3f}\",\n",
    "    \"nn_f1\":         f\"{np.mean(nn_f1):.3f}±{np.std(nn_f1):.3f}\",\n",
    "    \"nn_precision\":  f\"{np.mean(nn_prec):.3f}±{np.std(nn_prec):.3f}\",\n",
    "    \"nn_recall\":     f\"{np.mean(nn_rec):.3f}±{np.std(nn_rec):.3f}\",\n",
    "    \"nn_threshold\":  f\"{np.mean(nn_thr):.2f}\",\n",
    "\n",
    "    \"nn2_auc\":       f\"{np.mean(nn_auc2):.3f}±{np.std(nn_auc2):.3f}\",\n",
    "    \"nn2_f1\":        f\"{np.mean(nn_f12):.3f}±{np.std(nn_f12):.3f}\",\n",
    "    \"nn2_precision\": f\"{np.mean(nn_prec2):.3f}±{np.std(nn_prec2):.3f}\",\n",
    "    \"nn2_recall\":    f\"{np.mean(nn_rec2):.3f}±{np.std(nn_rec2):.3f}\",\n",
    "    \"nn2_threshold\": f\"{np.mean(nn_thr2):.2f}\",\n",
    "}\n",
    "\n",
    "dfOUT = pd.concat([dfOUT, pd.DataFrame([summary])], ignore_index=True)\n",
    "\n",
    "# ---- write to disk -----------------------------------------------------------\n",
    "out_file = r\"C:\\Users\\magst\\OneDrive\\Pictures\\Desktop\\Aviation\\FIGURES\\cv_results_haz.csv\"\n",
    "dfOUT.to_csv(out_file, index=False)\n",
    "print(f\"\\nSaved full results to:\\n  {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280b7ef5-f6d5-4e2e-ae20-230405f4eb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bf926-ce20-4c29-84a9-edf5478abb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bec941-9440-4d2a-b78f-cb09066f8fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b9f3f-bfff-40ec-916a-5963cf7764b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bbfd4-d62b-4798-b1e6-cda2644f6c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76606998-5d67-4f40-a233-d9f56286cc41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "622db893-f9dd-4402-b463-c7a5d9299dca",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d045a-8889-427d-ac5e-f1c7fd49e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, tensorflow as tf, keras_tuner as kt, tempfile, shutil, gc\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ─── DATA ──────────────────────────────────────────────────────────────────────\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# ─── CROSS‑VALIDATION SETUP ────────────────────────────────────────────────────\n",
    "N_SPLITS = 2\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "lr_auc, lr_f1, lr_prec, lr_rec, lr_thr   = [], [], [], [], []\n",
    "nn1_auc, nn1_f1, nn1_prec, nn1_rec, nn1_thr = [], [], [], [], []\n",
    "nn2_auc, nn2_f1, nn2_prec, nn2_rec, nn2_thr = [], [], [], [], []\n",
    "\n",
    "# ─── CUSTOM F1 METRIC FOR NN2 ─────────────────────────────────────────────────\n",
    "class F1Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "        recall    = self.tp / (self.tp + self.fn + 1e-8)\n",
    "        return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(0)\n",
    "\n",
    "# ─── LOOP OVER FOLDS ───────────────────────────────────────────────────────────\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train_df, X_test_df = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_idx].reset_index(drop=True), y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "    X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "    # ── Logistic Regression ───────────────────────────────────────────────────\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train_fold)\n",
    "    proba_lr = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "    auc_lr   = roc_auc_score(y_test_fold, proba_lr)\n",
    "    best_t_lr, best_f_lr = max(((t, f1_score(y_test_fold, proba_lr > t)) for t in np.linspace(0.01,0.99,99)), key=lambda x: x[1])\n",
    "    pred_lr  = proba_lr > best_t_lr\n",
    "    lr_auc.append(auc_lr); lr_f1.append(best_f_lr)\n",
    "    lr_prec.append(precision_score(y_test_fold, pred_lr)); lr_rec.append(recall_score(y_test_fold, pred_lr)); lr_thr.append(best_t_lr)\n",
    "\n",
    "    # class weights for NNs\n",
    "    cw = dict(zip(*np.unique(y_train_fold, return_counts=True)))\n",
    "    cw = {k: sum(cw.values())/v for k,v in cw.items()}\n",
    "\n",
    "    # ── NN1 (AUC only) ─────────────────────────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    def build_nn1(hp):\n",
    "        m = models.Sequential([\n",
    "            layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "            layers.Dense(hp.Int('u',16,256,16), activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid')])\n",
    "        m.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('lr',1e-4,1e-2,'log')),\n",
    "                  loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "        return m\n",
    "    tmp1 = tempfile.mkdtemp()\n",
    "    tuner1 = kt.RandomSearch(build_nn1, objective='val_auc', max_trials=5, directory=tmp1, project_name=f'nn1_{fold}')\n",
    "    tuner1.search(X_train_scaled, y_train_fold, epochs=5, batch_size=512, validation_split=0.2,\n",
    "                  callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)], class_weight=cw, verbose=0)\n",
    "    nn1 = tuner1.get_best_models(1)[0]\n",
    "    proba1 = nn1.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc1 = roc_auc_score(y_test_fold, proba1)\n",
    "    best_t1, best_f1 = max(((t, f1_score(y_test_fold, proba1 > t)) for t in np.linspace(0.01,0.99,99)), key=lambda x: x[1])\n",
    "    pred1 = proba1 > best_t1\n",
    "    nn1_auc.append(auc1); nn1_f1.append(best_f1)\n",
    "    nn1_prec.append(precision_score(y_test_fold, pred1)); nn1_rec.append(recall_score(y_test_fold, pred1)); nn1_thr.append(best_t1)\n",
    "    shutil.rmtree(tmp1, ignore_errors=True); gc.collect()\n",
    "\n",
    "    # ── NN2 (AUC + custom F1 metric) ───────────────────────────────────────────\n",
    "    tf.keras.backend.clear_session()\n",
    "    def build_nn2(hp):\n",
    "        m = models.Sequential([\n",
    "            layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "            layers.Dense(hp.Int('u',16,256,16), activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid')])\n",
    "        m.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('lr',1e-4,1e-2,'log')),\n",
    "                  loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()])\n",
    "        return m\n",
    "    tmp2 = tempfile.mkdtemp()\n",
    "    tuner2 = kt.RandomSearch(build_nn2, objective='val_auc', max_trials=5, directory=tmp2, project_name=f'nn2_{fold}')\n",
    "    tuner2.search(X_train_scaled, y_train_fold, epochs=2, batch_size=512, validation_split=0.2,\n",
    "                  callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)], class_weight=cw, verbose=0)\n",
    "    nn2 = tuner2.get_best_models(1)[0]\n",
    "    proba2 = nn2.predict(X_test_scaled, verbose=0).flatten()\n",
    "    auc2 = roc_auc_score(y_test_fold, proba2)\n",
    "    best_t2, best_f2 = max(((t, f1_score(y_test_fold, proba2 > t)) for t in np.linspace(0.01,0.99,99)), key=lambda x: x[1])\n",
    "    pred2 = proba2 > best_t2\n",
    "    nn2_auc.append(auc2); nn2_f1.append(best_f2)\n",
    "    nn2_prec.append(precision_score(y_test_fold, pred2)); nn2_rec.append(recall_score(y_test_fold, pred2)); nn2_thr.append(best_t2)\n",
    "    shutil.rmtree(tmp2, ignore_errors=True); gc.collect()\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "print(\"\\nLogistic Regression\")\n",
    "print(f\"  AUC {np.mean(lr_auc):.3f}±{np.std(lr_auc):.3f}  F1 {np.mean(lr_f1):.3f}±{np.std(lr_f1):.3f}  Prec {np.mean(lr_prec):.3f}  Rec {np.mean(lr_rec):.3f}\")\n",
    "\n",
    "print(\"\\nNeural Net 1 (AUC tuned)\")\n",
    "print(f\"  AUC {np.mean(nn1_auc):.3f}±{np.std(nn1_auc):.3f}  F1 {np.mean(nn1_f1):.3f}±{np.std(nn1_f1):.3f}  Prec {np.mean(nn1_prec):.3f}  Rec {np.mean(nn1_rec):.3f}\")\n",
    "\n",
    "print(\"\\nNeural Net 2 (AUC+F1 metric tuned)\")\n",
    "print(f\"  AUC {np.mean(nn2_auc):.3f}±{np.std(nn2_auc):.3f}  F1 {np.mean(nn2_f1):.3f}±{np.std(nn2_f1):.3f}  Prec {np.mean(nn2_prec):.3f}  Rec {np.mean(nn2_rec):.3f}\")\n",
    "\n",
    "# ─── SUMMARY ───────────────────────────────────────────────────────────────────\n",
    "def show(name, auc, f1, prec, rec, thr):\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  AUC        : {np.mean(auc):.3f} ± {np.std(auc):.3f}\")\n",
    "    print(f\"  F1         : {np.mean(f1):.3f} ± {np.std(f1):.3f}\")\n",
    "    print(f\"  Precision  : {np.mean(prec):.3f} ± {np.std(prec):.3f}\")\n",
    "    print(f\"  Recall     : {np.mean(rec):.3f} ± {np.std(rec):.3f}\")\n",
    "    print(f\"  Threshold  : {np.mean(thr):.2f} ± {np.std(thr):.2f}\")\n",
    "\n",
    "show(\"Logistic Regression (10‑fold CV)\",        lr_auc,  lr_f1,  lr_prec,  lr_rec,  lr_thr)\n",
    "show(\"Neural Net 1 (AUC tuned, 10‑fold CV)\",    nn1_auc, nn1_f1, nn1_prec, nn1_rec, nn1_thr)\n",
    "show(\"Neural Net 2 (AUC+F1 metric, 10‑fold CV)\",nn2_auc, nn2_f1, nn2_prec, nn2_rec, nn2_thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46857a6a-6790-4696-87de-f23536165b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic regression\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Prepare data\n",
    "# matched_clean = matched_clean.copy()\n",
    "# X = matched_clean[predictors]\n",
    "# y = matched_clean['event_type']\n",
    "# X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# # Scale features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "# X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "# # Class-weighted Logistic Regression\n",
    "# lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "# lr.fit(X_train_scaled, y_train)\n",
    "# y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# # AUC\n",
    "# print(\"AUC (class_weight LR):\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# # Find best threshold by F1\n",
    "# best_t, best_f = 0, 0\n",
    "# for t in [i / 100 for i in range(1, 100)]:\n",
    "#     f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "#     if f > best_f:\n",
    "#         best_f, best_t = f, t\n",
    "# print(f\"Best threshold (weighted): {best_t}, F1: {best_f}\")\n",
    "\n",
    "# # Classification report at best threshold\n",
    "# print(classification_report(y_test, (y_proba > best_t).astype(int)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Prepare data\n",
    "# matched_clean = matched_clean.copy()\n",
    "# X = matched_clean[predictors]\n",
    "# y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# # Set up cross-validation\n",
    "# skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# auc_scores = []\n",
    "# f1_scores = []\n",
    "# thresholds = []\n",
    "# reports = []\n",
    "\n",
    "# for train_idx, test_idx in skf.split(X, y):\n",
    "#     X_train_df = X.iloc[train_idx]\n",
    "#     X_test_df = X.iloc[test_idx]\n",
    "#     y_train = y.iloc[train_idx].reset_index(drop=True)\n",
    "#     y_test = y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "#     # Scale\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "#     X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "#     # Fit logistic regression\n",
    "#     lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "#     lr.fit(X_train_scaled, y_train)\n",
    "#     y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "#     # AUC\n",
    "#     auc = roc_auc_score(y_test, y_proba)\n",
    "#     auc_scores.append(auc)\n",
    "\n",
    "#     # Best F1 threshold\n",
    "#     best_t, best_f = 0, 0\n",
    "#     for t in [i / 100 for i in range(1, 100)]:\n",
    "#         f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "#         if f > best_f:\n",
    "#             best_f, best_t = f, t\n",
    "#     f1_scores.append(best_f)\n",
    "#     thresholds.append(best_t)\n",
    "\n",
    "#     # Save classification report (optional)\n",
    "#     y_pred = (y_proba > best_t).astype(int)\n",
    "#     reports.append(classification_report(y_test, y_pred, output_dict=True))\n",
    "\n",
    "# # Summary\n",
    "# print(f\"\\nMean AUC: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
    "# print(f\"Mean F1:  {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "# print(f\"Mean threshold: {np.mean(thresholds):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "matched_clean = matched_clean.copy()\n",
    "X = matched_clean[predictors]\n",
    "y = matched_clean['event_type'].reset_index(drop=True)\n",
    "\n",
    "# Set up cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "thresholds = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    X_train_df = X.iloc[train_idx]\n",
    "    X_test_df = X.iloc[test_idx]\n",
    "    y_train = y.iloc[train_idx].reset_index(drop=True)\n",
    "    y_test = y.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_df), columns=X.columns, index=X_train_df.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test_df), columns=X.columns, index=X_test_df.index)\n",
    "\n",
    "    # Fit logistic regression\n",
    "    lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    y_proba = lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # AUC\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "    # Best F1 threshold\n",
    "    best_t, best_f = 0, 0\n",
    "    for t in np.linspace(0.01, 0.99, 99):\n",
    "        f = f1_score(y_test, (y_proba > t).astype(int))\n",
    "        if f > best_f:\n",
    "            best_f, best_t = f, t\n",
    "    thresholds.append(best_t)\n",
    "    f1_scores.append(best_f)\n",
    "\n",
    "    # Compute precision and recall at best threshold\n",
    "    y_pred = (y_proba > best_t).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "\n",
    "    # report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # # Collect class-specific metrics\n",
    "    # precision_scores_class0.append(report['0']['precision'])\n",
    "    # recall_scores_class0.append(report['0']['recall'])\n",
    "    # f1_scores_class0.append(report['0']['f1-score'])\n",
    "    \n",
    "    # precision_scores_class1.append(report['1']['precision'])\n",
    "    # recall_scores_class1.append(report['1']['recall'])\n",
    "    # f1_scores_class1.append(report['1']['f1-score'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Summary\n",
    "print(f\"\\nMean AUC:       {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
    "print(f\"Mean F1:        {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "print(f\"Mean Precision: {np.mean(precision_scores):.3f} ± {np.std(precision_scores):.3f}\")\n",
    "print(f\"Mean Recall:    {np.mean(recall_scores):.3f} ± {np.std(recall_scores):.3f}\")\n",
    "print(f\"Mean Threshold: {np.mean(thresholds):.2f}\")\n",
    "# print(\"\\nClass 0 (non-event):\")\n",
    "# print(f\"  Precision: {np.mean(precision_scores_class0):.3f}\")\n",
    "# print(f\"  Recall:    {np.mean(recall_scores_class0):.3f}\")\n",
    "# print(f\"  F1 Score:  {np.mean(f1_scores_class0):.3f}\")\n",
    "\n",
    "# print(\"\\nClass 1 (event):\")\n",
    "# print(f\"  Precision: {np.mean(precision_scores_class1):.3f}\")\n",
    "# print(f\"  Recall:    {np.mean(recall_scores_class1):.3f}\")\n",
    "# print(f\"  F1 Score:  {np.mean(f1_scores_class1):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e966dd-6d22-4dfc-815f-2f2af8fb49b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "# --- Custom F1 metric ---------------------------------------------------------\n",
    "class F1Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "        recall   = self.tp / (self.tp + self.fn + 1e-8)\n",
    "        return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(0)\n",
    "\n",
    "# --- Model builder -----------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    hp_units = hp.Int('units', min_value=16, max_value=256, step=16)\n",
    "    hp_lr    = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(hp_units, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Hyperparameter tuning ----------------------------------------------------\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=20,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='one_hidden_layer_6'\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluation ---------------------------------------------------------------\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Print model architecture\n",
    "best_model.summary()\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print all tuned hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for param in best_hp.values:\n",
    "    print(f\"{param}: {best_hp.get(param)}\")\n",
    "\n",
    "    \n",
    "\n",
    "y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "print(\"AUC (NN):\", roc_auc_score(y_test, y_proba_nn))\n",
    "\n",
    "best_t_nn, best_f_nn = 0, 0\n",
    "for t in np.linspace(0.01, 0.99, 99):\n",
    "    f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "    if f > best_f_nn:\n",
    "        best_f_nn, best_t_nn = f, t\n",
    "\n",
    "print(f\"Best threshold (NN): {best_t_nn}, F1: {best_f_nn}\")\n",
    "print(classification_report(y_test, (y_proba_nn > best_t_nn).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586eb48-f0cc-4527-8a2e-7d0e331b7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "\n",
    "# --- Custom F1 metric ---------------------------------------------------------\n",
    "class F1Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.fn = self.add_weight(name='fn', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        self.tp.assign_add(tf.reduce_sum(y_true * y_pred))\n",
    "        self.fp.assign_add(tf.reduce_sum((1 - y_true) * y_pred))\n",
    "        self.fn.assign_add(tf.reduce_sum(y_true * (1 - y_pred)))\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
    "        recall   = self.tp / (self.tp + self.fn + 1e-8)\n",
    "        return 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    def reset_states(self):\n",
    "        for v in self.variables:\n",
    "            v.assign(0)\n",
    "\n",
    "# --- Model builder -----------------------------------------------------------\n",
    "# --- Model builder -----------------------------------------------------------\n",
    "def build_model(hp):\n",
    "    hp_units_1 = hp.Int('units_1', min_value=16, max_value=256, step=16)\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=256, step=16)\n",
    "    hp_lr      = hp.Float('lr', 1e-4, 1e-2, sampling='log')\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "        layers.Dense(hp_units_1, activation='relu'),\n",
    "        layers.Dense(hp_units_2, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc'), F1Metric()]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Hyperparameter tuning ----------------------------------------------------\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_auc',\n",
    "    max_trials=20,\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='two_hidden_layers_test2'\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=512,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluation ---------------------------------------------------------------\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Print model architecture\n",
    "best_model.summary()\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Print all tuned hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for param in best_hp.values:\n",
    "    print(f\"{param}: {best_hp.get(param)}\")\n",
    "\n",
    "    \n",
    "\n",
    "y_proba_nn = best_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "print(\"AUC (NN):\", roc_auc_score(y_test, y_proba_nn))\n",
    "\n",
    "best_t_nn, best_f_nn = 0, 0\n",
    "for t in np.linspace(0.01, 0.99, 99):\n",
    "    f = f1_score(y_test, (y_proba_nn > t).astype(int))\n",
    "    if f > best_f_nn:\n",
    "        best_f_nn, best_t_nn = f, t\n",
    "\n",
    "print(f\"Best threshold (NN): {best_t_nn}, F1: {best_f_nn}\")\n",
    "print(classification_report(y_test, (y_proba_nn > best_t_nn).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3317820b-a4ca-47cf-b187-41adbbba61d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd29c43-1897-44a3-b2e7-b97306291213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
